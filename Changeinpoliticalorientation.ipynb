{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change in political orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisieren der Verbindung zu ER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install eventregistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eventregistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete max_Items before doing final statistics\n",
    "\n",
    "#this code returns: big_events_energy; big_events_immigration; big_events_ukraine\n",
    "\n",
    "max_Items=200            #set how many events ER should iterate through. There is no default value. If not used, then ER iterates until results run out.\n",
    "total_results=5         #set how many event dates should be delivered back (only takes the events with the most relevance)\n",
    "min_articles=80        #set the minimum amount of articles, that each event should have\n",
    "min_relevance=1       #set how well the event has to fit the query at least\n",
    "\n",
    "from eventregistry import *\n",
    "\n",
    "# Initialize EventRegistry with SOSEC API key\n",
    "er = EventRegistry(apiKey=\"be75941d-07ae-406a-ad2d-9d3cfe44c892\")\n",
    "\n",
    "#energy crisis\n",
    "# Query parameters for events related to the energy crisis with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "            {\"conceptUri\": \"http://en.wikipedia.org/wiki/Energy_crisis\"},     # Focus on energy crisis, not energy drinks --> concepts\n",
    "            #{\"$or\":[\n",
    "            #    {\"keyword\": \"crisis\"},\n",
    "            #    {\"keyword\": \"energy\"}\n",
    "            #    ]},\n",
    "\n",
    "            #{\"keyword\": \"energy crisis\"},\n",
    "            {\"categoryUri\": \"dmoz/Business/Energy\"},\n",
    "            #{\"$or\":[\n",
    "            #        {\"conceptUri\": \"http://en.wikipedia.org/wiki/Energy\"},        #conceptUri = er.getConceptUri(\"energy\")\n",
    "            #        {\"conceptUri\": \"http://en.wikipedia.org/wiki/Energy_security\"},\n",
    "            #]},\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\n",
    "        \"minRelevanceScore\": min_relevance,         # Set minimum relevance score to 50\n",
    "        \"includeEventSentiment\": True,         # Include sentiment in the response\n",
    "        \"includeEventDate\": True,              # Include event date in the response\n",
    "        \"includeEventArticleCounts\": True,     # Include article counts to measure coverage\n",
    "        \"includeEventRelevanceScore\": True,\n",
    "        \"lang\": \"eng\"                          # Sentiment only calculated for English articles\n",
    "    },\n",
    "    #Important: relevance is the relevance to the query, not the relevance of the event itself!!!\n",
    "    #\"sortBy\": \"rel\",     # Sort by relevance\n",
    "    #\"sortByAsc\": False    # Sort in descending order (most relevant first)\n",
    "}\n",
    "\n",
    "# Create a list to store event dates\n",
    "events_energy_list = []\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and retrieve events\n",
    "for event in q.execQuery(er, maxItems=max_Items, sortBy=\"relevance\"):  # Increase maxItems to get more events\n",
    "\n",
    "\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    relevance_score = event.get(\"relevance\", \"No Score\")  # Retrieve relevance score\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    event_title = event.get(\"title\", {}).get(\"eng\", \"No Title\")  # Extract only the English title  #maybe change later\n",
    "    event_sentiment = event.get(\"sentiment\", \"Not found\")       # maybe use later:negative_sentiment = event.get(\"negative\", \"Not found\")\n",
    "    event_uri = event.get(\"uri\", \"No uri Found\")  # Get the event's ID for H11\n",
    "\n",
    "    events_energy_list.append({\"date\": event_date, \"article_count\": article_count, \"uri\": event_uri, \"title\": event_title, \"relevance\": relevance_score})\n",
    "\n",
    "\n",
    "# Sort events based on the total number of articles in descending order and Store results for Chow test. For now: Print sorted results\n",
    "big_events_energy = sorted(events_energy_list, key=lambda x: x[\"relevance\"], reverse=True)[:total_results]    #only keep the biggest events\n",
    "\n",
    "#other option: minimum article count# Filter events to keep only those with article count over 50\n",
    "#OptionalZuVergeben = [event for event in big_events_energy if event[\"article_count\"] > 50]\n",
    "\n",
    "for event in big_events_energy:\n",
    "    print(f\"{event['date']}, {event['title']}\", {event['article_count']})\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "#by repeating the query for rising immigration and the war on Ukraine,\n",
    "#we identify, which relevant event is responsible for a change in political orientation\n",
    "\n",
    "\n",
    "\n",
    "#rising immigration\n",
    "#\n",
    "# Query parameters for events related to the rising immigration with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "            {\"$or\": [\n",
    "                {\"keyword\": \"rising immigration\"},\n",
    "                {\"keyword\": \"growing immigration\"},\n",
    "                {\"keyword\": \"Surge in immigration\"},\n",
    "                {\"keyword\": \"Immigration growth\"},\n",
    "                {\"keyword\": \"migrant population\"},\n",
    "                {\"keyword\": \"trend in immigration\"},\n",
    "                {\"keyword\": \"migrant arrivals\"},\n",
    "            ]},\n",
    "            {\"categoryUri\": \"dmoz/Society/Issues/Immigration\"},\n",
    "            {\"conceptUri\": er.getConceptUri(\"immigration\")},           #http://en.wikipedia.org/wiki/Immigration\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\n",
    "        \"minSentiment\": -1,\n",
    "        \"maxSentiment\": 1,             # Maximum sentiment value (-0.5 for negative sentiment)\n",
    "        \"minRelevanceScore\": min_relevance,         # Set minimum relevance score to 50\n",
    "        \"includeEventSentiment\": True,         # Include sentiment in the response\n",
    "        \"includeEventDate\": True,              # Include event date in the response\n",
    "        \"includeEventArticleCounts\": True,     # Include article counts to measure coverage\n",
    "        \"includeEventRelevanceScore\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create a list to store event dates\n",
    "events_immigration_list = []\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and retrieve events\n",
    "for event in q.execQuery(er, maxItems=max_Items, sortBy=\"relevance\"):  # Increase maxItems to get more events\n",
    "\n",
    "\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    relevance_score = event.get(\"relevance\", \"No Score\")  # Retrieve relevance score\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    event_title = event.get(\"title\", {}).get(\"eng\", \"No Title\")  # Extract only the English title  #maybe change later\n",
    "    event_sentiment = event.get(\"sentiment\", \"Not found\")       # maybe use later:negative_sentiment = event.get(\"negative\", \"Not found\")\n",
    "\n",
    "    events_immigration_list.append({\"date\": event_date, \"article_count\": article_count, \"title\": event_title, \"relevance\": relevance_score})\n",
    "\n",
    "\"\"\"\n",
    "    print(event)          #First line of output   #just to check other parameters   #might delete later\n",
    "    print(f\"Event Title: {event_title}\")\n",
    "    print(f\"Event Date: {event_date}\")\n",
    "    print(f\"Relevance Score: {relevance_score}\")\n",
    "    print(f\"Total Articles: {article_count}\")\n",
    "    print(f\"Event Sentiment: {event_sentiment}\")\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\"\"\"\n",
    "\n",
    "# Sort events based on the total number of articles in descending order and store results for Chow test.\n",
    "# only keep the biggest events [:number]\n",
    "#For now: Print sorted results\n",
    "big_events_immigration = sorted(events_immigration_list, key=lambda x: x[\"relevance\"], reverse=True)[:total_results]\n",
    "for event in big_events_immigration:\n",
    "    print(f\"{event['date']}, {event['title']}\", {event['article_count']})\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#war on Ukraine\n",
    "#\n",
    "# Query parameters for events related to the war on Ukraine with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "            {\"keyword\": \"war on Ukraine\"},                                   # Already very specific. Barely room for misunderstanding\n",
    "            {\"categoryUri\": \"news/Politics\"},\n",
    "            #after inspecting the articles, there was one about the death of taliban head something.... But this may also correlate to the war on Ukraine.\n",
    "            #{\"categoryUri\": \"dmoz/Society/Issues/Warfare_and_Conflict\"},     # Further investigate filters\n",
    "            #{\"conceptUri\": \"http://en.wikipedia.org/wiki/War_in_Donbass\"},   # No concept fits the current war on Ukraine well\n",
    "            #{\"conceptUri\": er.getConceptUri(\"war on Ukraine\")},              # returns \"Red Famine: Stalin's War on Ukraine\"\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\n",
    "        \"minSentiment\": -1,\n",
    "        \"maxSentiment\": 1,             # Maximum sentiment value\n",
    "        \"minRelevanceScore\": min_relevance,         # Set minimum relevance score to smth\n",
    "        \"includeEventSentiment\": True,         # Include sentiment in the response\n",
    "        \"includeEventDate\": True,              # Include event date in the response\n",
    "        \"includeEventArticleCounts\": True,     # Include article counts to measure coverage\n",
    "        \"includeEventRelevanceScore\": True,\n",
    "        \"lang\": \"eng\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create a list to store event dates\n",
    "events_ukraine_list = []\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and retrieve events\n",
    "for event in q.execQuery(er, maxItems=max_Items, sortBy=\"relevance\"):  # Increase maxItems to get more events\n",
    "\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    relevance_score = event.get(\"relevance\", \"No Score\")  # Retrieve relevance score\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    event_title = event.get(\"title\", {}).get(\"eng\", \"No english Title\")  # Extract only the English title  #maybe change later\n",
    "    event_sentiment = event.get(\"sentiment\", \"Not found\")       # maybe use later:negative_sentiment = event.get(\"negative\", \"Not found\")\n",
    "\n",
    "    events_ukraine_list.append({\"date\": event_date, \"article_count\": article_count, \"title\": event_title, \"relevance\": relevance_score})\n",
    "\n",
    "    \"\"\"\n",
    "    #might delete later\n",
    "    print(event)\n",
    "    print(f\"Event Title: {event_title}\")\n",
    "    \"\"\"\n",
    "\n",
    "# Sort events based on the total number of articles in descending order and store results for Chow test.\n",
    "# only keep the biggest events [:number]\n",
    "#For now: Print sorted results\n",
    "big_events_ukraine = sorted(events_ukraine_list, key=lambda x: x[\"relevance\"], reverse=True)[:total_results]\n",
    "for event in big_events_ukraine:\n",
    "    print(f\"{event['date']}, {event['title']}\", {event['article_count']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in big_events_energy:\n",
    "    print(event)\n",
    "#Die Liste wurde durch abbrechen der Verbindung zum Server geleert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventregistry import *\n",
    "\n",
    "max_Items=20            #set how many events ER should iterate through. Set low to minimize use of tokens\n",
    "total_results=5         #set how many event dates should be delivered back (only takes the events with the most articles)\n",
    "min_articles=100        #set the minimum amount of articles, that each event should have\n",
    "min_relevance=50        #set how well the event has to fit the query\n",
    "\n",
    "#this code returns: big_events_energy; big_events_immigration; big_events_ukraine\n",
    "\n",
    "# Initialize EventRegistry with SOSEC API key\n",
    "er = EventRegistry(apiKey=\"be75941d-07ae-406a-ad2d-9d3cfe44c892\")\n",
    "\n",
    "#energy crisis\n",
    "# Query parameters for events related to the energy crisis with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "            {\"conceptUri\": \"http://en.wikipedia.org/wiki/Energy_crisis\"},     # Focus on energy crisis, not energy drinks --> concepts\n",
    "            #{\"$or\":[\n",
    "            #    {\"keyword\": \"crisis\"},                                        # somehow relevant in retrieving title\n",
    "            #    {\"keyword\": \"energy\"}\n",
    "            #    ]},\n",
    "\n",
    "            #{\"keyword\": \"energy crisis\"},\n",
    "            {\"categoryUri\": \"dmoz/Business/Energy\"},\n",
    "            #{\"$or\":[\n",
    "            #        {\"conceptUri\": \"http://en.wikipedia.org/wiki/Energy\"},        #conceptUri = er.getConceptUri(\"energy\")\n",
    "            #        {\"conceptUri\": \"http://en.wikipedia.org/wiki/Energy_security\"},\n",
    "            #]},\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\n",
    "        \"minRelevanceScore\": min_relevance,         # Set minimum relevance score to 50\n",
    "        \"includeEventSentiment\": True,         # Include sentiment in the response\n",
    "        \"includeEventDate\": True,              # Include event date in the response\n",
    "        \"includeEventArticleCounts\": True,     # Include article counts to measure coverage\n",
    "        \"includeEventRelevanceScore\": True,\n",
    "        \"lang\": \"eng\"                          # Sentiment only calculated for English articles\n",
    "    },\n",
    "    }\n",
    "\n",
    "# Create a list to store event dates\n",
    "events_energy_list = []\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and retrieve events\n",
    "for event in q.execQuery(er, maxItems=max_Items):  # Increase maxItems to get more events\n",
    "\n",
    "\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    relevance_score = event.get(\"relevance\", \"No Score\")  # Retrieve relevance score\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    event_title = event.get(\"title\", {}).get(\"eng\", \"No Title\")  # Extract only the English title  #maybe change later\n",
    "    event_sentiment = event.get(\"sentiment\", \"Not found\")       # maybe use later:negative_sentiment = event.get(\"negative\", \"Not found\")\n",
    "    event_uri = event.get(\"uri\", \"No uri Found\")  # Get the event's ID for H11\n",
    "\n",
    "    events_energy_list.append({\"date\": event_date, \"article_count\": article_count, \"uri\": event_uri, \"title\": event_title})\n",
    "\n",
    "\n",
    "# Sort events based on the total number of articles in descending order and Store results for Chow test. For now: Print sorted results\n",
    "big_events_energy = sorted(events_energy_list, key=lambda x: x[\"article_count\"], reverse=True)[:total_results]    #only keep the biggest events\n",
    "\n",
    "#other option: minimum article count# Filter events to keep only those with article count over 50\n",
    "#OptionalZuVergeben = [event for event in big_events_energy if event[\"article_count\"] > 50]\n",
    "\n",
    "for event in big_events_energy:\n",
    "    print(f\"{event['date']}, {event['title']}\")\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "#by repeating the query for rising immigration and the war on Ukraine,\n",
    "#we identify, which relevant event is responsible for a change in political orientation\n",
    "\n",
    "\n",
    "\n",
    "#rising immigration\n",
    "#\n",
    "# Query parameters for events related to the rising immigration with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "            {\"$or\": [\n",
    "                {\"keyword\": \"rising immigration\"},\n",
    "                {\"keyword\": \"growing immigration\"},\n",
    "                {\"keyword\": \"Surge in immigration\"},\n",
    "                {\"keyword\": \"Immigration growth\"},\n",
    "                {\"keyword\": \"migrant population\"},\n",
    "                {\"keyword\": \"trend in immigration\"},\n",
    "                {\"keyword\": \"migrant arrivals\"},\n",
    "            ]},\n",
    "            {\"categoryUri\": \"dmoz/Society/Issues/Immigration\"},\n",
    "            {\"conceptUri\": er.getConceptUri(\"immigration\")},           #http://en.wikipedia.org/wiki/Immigration\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\n",
    "        \"minSentiment\": -1,\n",
    "        \"maxSentiment\": 1,             # Maximum sentiment value (-0.5 for negative sentiment)\n",
    "        \"minRelevanceScore\": min_relevance,         # Set minimum relevance score to 50\n",
    "        \"includeEventSentiment\": True,         # Include sentiment in the response\n",
    "        \"includeEventDate\": True,              # Include event date in the response\n",
    "        \"includeEventArticleCounts\": True,     # Include article counts to measure coverage\n",
    "        \"includeEventRelevanceScore\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create a list to store event dates\n",
    "events_immigration_list = []\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and retrieve events\n",
    "for event in q.execQuery(er, maxItems=max_Items):  # Increase maxItems to get more events\n",
    "\n",
    "\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    relevance_score = event.get(\"relevance\", \"No Score\")  # Retrieve relevance score\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    event_title = event.get(\"title\", {}).get(\"eng\", \"No Title\")  # Extract only the English title  #maybe change later\n",
    "    event_sentiment = event.get(\"sentiment\", \"Not found\")       # maybe use later:negative_sentiment = event.get(\"negative\", \"Not found\")\n",
    "\n",
    "    events_immigration_list.append({\"date\": event_date, \"article_count\": article_count, \"title\": event_title})\n",
    "\n",
    "\"\"\"\n",
    "    print(event)          #First line of output   #just to check other parameters   #might delete later\n",
    "    print(f\"Event Title: {event_title}\")\n",
    "    print(f\"Event Date: {event_date}\")\n",
    "    print(f\"Relevance Score: {relevance_score}\")\n",
    "    print(f\"Total Articles: {article_count}\")\n",
    "    print(f\"Event Sentiment: {event_sentiment}\")\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\"\"\"\n",
    "\n",
    "# Sort events based on the total number of articles in descending order and store results for Chow test.\n",
    "# only keep the biggest events [:number]\n",
    "#For now: Print sorted results\n",
    "big_events_immigration = sorted(events_immigration_list, key=lambda x: x[\"article_count\"], reverse=True)[:total_results]\n",
    "for event in big_events_immigration:\n",
    "    print(f\"{event['date']}, {event['title']}\")\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#war on Ukraine\n",
    "#\n",
    "# Query parameters for events related to the war on Ukraine with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "            {\"keyword\": \"war on Ukraine\"},                                   # Already very specific. No room for misunderstanding\n",
    "            {\"categoryUri\": \"news/Politics\"},\n",
    "            #after inspecting the articles, there was one about the death of taliban head something.... But this may also correlate to the war on Ukraine.\n",
    "            #{\"categoryUri\": \"dmoz/Society/Issues/Warfare_and_Conflict\"},     # Further investigate filters\n",
    "            #{\"conceptUri\": \"http://en.wikipedia.org/wiki/War_in_Donbass\"},   # No concept fits the current war on Ukraine well\n",
    "            #{\"conceptUri\": er.getConceptUri(\"war on Ukraine\")},              # returns \"Red Famine: Stalin's War on Ukraine\"\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\n",
    "        \"minSentiment\": -1,\n",
    "        \"maxSentiment\": 1,             # Maximum sentiment value\n",
    "        \"minRelevanceScore\": min_relevance,         # Set minimum relevance score to 50\n",
    "        \"includeEventSentiment\": True,         # Include sentiment in the response\n",
    "        \"includeEventDate\": True,              # Include event date in the response\n",
    "        \"includeEventArticleCounts\": True,     # Include article counts to measure coverage\n",
    "        \"includeEventRelevanceScore\": True,\n",
    "        \"lang\": \"eng\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create a list to store event dates\n",
    "events_ukraine_list = []\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and retrieve events\n",
    "for event in q.execQuery(er, maxItems=max_Items):  # Increase maxItems to get more events\n",
    "\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    relevance_score = event.get(\"relevance\", \"No Score\")  # Retrieve relevance score\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    event_title = event.get(\"title\", {}).get(\"eng\", \"No english Title\")  # Extract only the English title  #maybe change later\n",
    "    event_sentiment = event.get(\"sentiment\", \"Not found\")       # maybe use later:negative_sentiment = event.get(\"negative\", \"Not found\")\n",
    "\n",
    "    events_ukraine_list.append({\"date\": event_date, \"article_count\": article_count, \"title\": event_title})\n",
    "\n",
    "    \"\"\"\n",
    "    #might delete later\n",
    "    print(event)\n",
    "    print(f\"Event Title: {event_title}\")\n",
    "    \"\"\"\n",
    "\n",
    "# Sort events based on the total number of articles in descending order and store results for Chow test.\n",
    "# only keep the biggest events [:number]\n",
    "#For now: Print sorted results\n",
    "big_events_ukraine = sorted(events_ukraine_list, key=lambda x: x[\"article_count\"], reverse=True)[:total_results]\n",
    "for event in big_events_ukraine:\n",
    "    print(f\"{event['date']}, {event['title']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventregistry import *\n",
    "\n",
    "max_Items=30            #set how many events ER should iterate through. Set low to minimize use of tokens\n",
    "total_results=5         #set how many event dates should be delivered back (only takes the events with the most articles)\n",
    "min_articles=50        #set the minimum amount of articles, that each event should have  #might delete later\n",
    "min_relevance=50        #set how well the event has to fit the query\n",
    "\n",
    "#this code returns: negative_events_energy; negative_events_immigration; negative_events_ukraine\n",
    "\n",
    "# Initialize EventRegistry with SOSEC API key\n",
    "er = EventRegistry(apiKey=\"be75941d-07ae-406a-ad2d-9d3cfe44c892\")\n",
    "\n",
    "# Create an instance of ReturnInfo to specify the properties you want\n",
    "return_info = ReturnInfo(eventInfo=EventInfoFlags(socialScore=True, articleCounts=True, date=True))\n",
    "\n",
    "\n",
    "#energy crisis\n",
    "# Query parameters for events related to the energy crisis with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "            {\"conceptUri\": \"http://en.wikipedia.org/wiki/Energy_crisis\"},     # Focus on energy crisis, not energy drinks --> concepts\n",
    "            #{\"$or\":[\n",
    "            #    {\"keyword\": \"crisis\"},                                        # somehow relevant in retrieving title\n",
    "            #    {\"keyword\": \"energy\"}\n",
    "            #    ]},\n",
    "\n",
    "            #{\"keyword\": \"energy crisis\"},\n",
    "            {\"categoryUri\": \"dmoz/Business/Energy\"},\n",
    "            #{\"$or\":[\n",
    "            #        {\"conceptUri\": \"http://en.wikipedia.org/wiki/Energy\"},        #conceptUri = er.getConceptUri(\"energy\")\n",
    "            #        {\"conceptUri\": \"http://en.wikipedia.org/wiki/Energy_security\"},\n",
    "            #]},\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\"minRelevanceScore\": min_relevance},\n",
    "    }\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and store results\n",
    "events = []\n",
    "for event in q.execQuery(er, returnInfo=return_info, maxItems=max_Items):\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    sentiment = event.get(\"sentiment\", {})  # Get the sentiment score (negative or positive)\n",
    "    events.append({\n",
    "        \"date\": event_date,\n",
    "        \"article_count\": article_count,\n",
    "        \"sentiment\": sentiment\n",
    "    })\n",
    "\n",
    "\n",
    "# Sort events based on the sentiment score, with the most negative scores at the top\n",
    "negative_events_energy = sorted(events, key=lambda x: x[\"sentiment\"])\n",
    "\n",
    "\n",
    "# Print the events with the most negative sentiment\n",
    "print(\"Energy events with the lowest sentiment:\")\n",
    "for event in negative_events_energy[:5]:\n",
    "    print(f\"Date: {event['date']}, Article Count: {event['article_count']}, Sentiment: {event['sentiment']}\")\n",
    "\n",
    "\n",
    "\n",
    "#rising immigration\n",
    "#\n",
    "# Query parameters for events related to the rising immigration with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "            {\"$or\": [\n",
    "                {\"keyword\": \"rising immigration\"},\n",
    "                {\"keyword\": \"growing immigration\"},\n",
    "                {\"keyword\": \"Surge in immigration\"},\n",
    "                {\"keyword\": \"Immigration growth\"},\n",
    "                {\"keyword\": \"migrant population\"},\n",
    "                {\"keyword\": \"trend in immigration\"},\n",
    "                {\"keyword\": \"migrant arrivals\"},\n",
    "            ]},\n",
    "            {\"categoryUri\": \"dmoz/Society/Issues/Immigration\"},\n",
    "            {\"conceptUri\": er.getConceptUri(\"immigration\")},           #http://en.wikipedia.org/wiki/Immigration\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\n",
    "        \"minRelevanceScore\": min_relevance,         # Set minimum relevance score\n",
    "    },}\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and store results\n",
    "events = []\n",
    "for event in q.execQuery(er, returnInfo=return_info, maxItems=max_Items):\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    sentiment = event.get(\"sentiment\", {})  # Get the sentiment score (negative or positive)\n",
    "    events.append({\n",
    "        \"date\": event_date,\n",
    "        \"article_count\": article_count,\n",
    "        \"sentiment\": sentiment\n",
    "    })\n",
    "\n",
    "\n",
    "# Sort events based on the sentiment score, with the most negative scores at the top\n",
    "negative_events_immigration = sorted(events, key=lambda x: x[\"sentiment\"])\n",
    "\n",
    "\n",
    "# Print the events with the most negative sentiment\n",
    "print(\"Immigration events with the lowest sentiment:\")\n",
    "for event in negative_events_immigration[:5]:\n",
    "    print(f\"Date: {event['date']}, Article Count: {event['article_count']}, Sentiment: {event['sentiment']}\")\n",
    "\n",
    "\n",
    "\n",
    "#war on Ukraine\n",
    "#\n",
    "# Query parameters for events related to the war on Ukraine with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "            {\"keyword\": \"war on Ukraine\"},                                   # Already very specific. No room for misunderstanding\n",
    "            {\"categoryUri\": \"news/Politics\"},\n",
    "            #after inspecting the articles, there was one about the death of taliban head something.... But this may also correlate to the war on Ukraine.\n",
    "            #{\"categoryUri\": \"dmoz/Society/Issues/Warfare_and_Conflict\"},     # Further investigate filters\n",
    "            #{\"conceptUri\": \"http://en.wikipedia.org/wiki/War_in_Donbass\"},   # No concept fits the current war on Ukraine well\n",
    "            #{\"conceptUri\": er.getConceptUri(\"war on Ukraine\")},              # returns \"Red Famine: Stalin's War on Ukraine\"\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\n",
    "        \"minRelevanceScore\": min_relevance,         # Set minimum relevance score\n",
    "    },}\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and store results\n",
    "events = []\n",
    "for event in q.execQuery(er, returnInfo=return_info, maxItems=max_Items):\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    sentiment = event.get(\"sentiment\", {})  # Get the sentiment score (negative or positive)\n",
    "\n",
    "    # Ensure sentiment is a valid number\n",
    "    if sentiment is None or (sentiment < -1 or sentiment > 1):\n",
    "        print(\"ScheiÃŸe\")\n",
    "\n",
    "    events.append({\n",
    "        \"date\": event_date,\n",
    "        \"article_count\": article_count,\n",
    "        \"sentiment\": sentiment\n",
    "    })\n",
    "\n",
    "\n",
    "# Sort events based on the sentiment score, with the most negative scores at the top\n",
    "negative_events_Ukraine = sorted(events, key=lambda x: x[\"sentiment\"])\n",
    "\n",
    "\n",
    "# Print the events with the most negative sentiment\n",
    "print(\"Ukraine events with the lowest sentiment:\")\n",
    "for event in negative_events_Ukraine[:5]:\n",
    "    print(f\"Date: {event['date']}, Article Count: {event['article_count']}, Sentiment: {event['sentiment']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventregistry import *\n",
    "\n",
    "max_Items=50            #set how many events ER should iterate through. Set low to minimize use of tokens\n",
    "total_results=5         #set how many event dates should be delivered back (only takes the events with the most articles)\n",
    "min_articles=20        #set the minimum amount of articles, that each event should have  #might delete later\n",
    "min_relevance=50        #set how well the event has to fit the query\n",
    "\n",
    "#this code returns: social_events_energy; social_events_immigration; social_events_ukraine\n",
    "\n",
    "# Initialize EventRegistry with SOSEC key\n",
    "er = EventRegistry(apiKey=\"be75941d-07ae-406a-ad2d-9d3cfe44c892\")\n",
    "\n",
    "\n",
    "# Create an instance of ReturnInfo to specify the properties you want\n",
    "return_info = ReturnInfo(eventInfo=EventInfoFlags(socialScore=True, articleCounts=True, date=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#energy crisis\n",
    "# Query parameters for events related to the energy crisis with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "            {\"conceptUri\": \"http://en.wikipedia.org/wiki/Energy_crisis\"},     # Focus on energy crisis, not energy drinks --> concepts\n",
    "            {\"$or\":[\n",
    "                {\"keyword\": \"crisis\"},\n",
    "                {\"keyword\": \"energy\"}\n",
    "                ]},\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\n",
    "        \"minRelevanceScore\": min_relevance,         # Set minimum relevance score\n",
    "        \"includeEventSentiment\": True,         # Include sentiment in the response\n",
    "        \"includeEventDate\": True,              # Include event date in the response\n",
    "        \"includeEventArticleCounts\": True,     # Include article counts to measure coverage\n",
    "        \"includeEventRelevanceScore\": True,\n",
    "    },}\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and store results\n",
    "events = []\n",
    "for event in q.execQuery(er,returnInfo=return_info, maxItems=100):  # added returnInfo\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    social_score = event.get(\"socialScore\", 1)  # Ensure this field exists and avoid division by zero\n",
    "    if article_count > 0:  # Avoid division by zero\n",
    "        ratio = social_score / article_count  # Ratio of social score to article count\n",
    "        events.append({\n",
    "            \"date\": event_date,\n",
    "            \"article_count\": article_count,\n",
    "            \"social_score\": social_score,\n",
    "            \"ratio\": ratio\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "# Sort events based on the ratio of article count to social score\n",
    "social_events_energy = sorted(events, key=lambda x: x[\"ratio\"], reverse=True)    #changed filtered_events to events\n",
    "\n",
    "#felt cute, might delete later\n",
    "# Print events with the highest and lowest ratios\n",
    "print(\"Energy events with high media coverage (Social Score / Article Count):\")\n",
    "for event in social_events_energy[:5]:\n",
    "    print(f\"Date: {event['date']}, Social Score: {event['social_score']}, Article Count: {event['article_count']}, Ratio: {event['ratio']}\")\n",
    "\n",
    "print(\"Energy events with low media coverage (Social Score / Article Count):\")\n",
    "for event in social_events_energy[-5:]:\n",
    "    print(f\"Date: {event['date']}, Social Score: {event['social_score']}, Article Count: {event['article_count']}, Ratio: {event['ratio']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#rising immigration\n",
    "#\n",
    "# Query parameters for events related to the rising immigration with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "             {\"$or\": [\n",
    "                {\"keyword\": \"rising immigration\"},\n",
    "                {\"keyword\": \"growing immigration\"},\n",
    "                {\"keyword\": \"Surge in immigration\"},\n",
    "                {\"keyword\": \"Immigration growth\"},\n",
    "                {\"keyword\": \"migrant population\"},\n",
    "                {\"keyword\": \"trend in immigration\"},\n",
    "                {\"keyword\": \"migrant arrivals\"},\n",
    "            ]},\n",
    "            {\"categoryUri\": \"dmoz/Society/Issues/Immigration\"},\n",
    "            {\"conceptUri\": er.getConceptUri(\"immigration\")},\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\n",
    "        \"minRelevanceScore\": min_relevance,         # Set minimum relevance score\n",
    "        \"includeEventSentiment\": True,         # Include sentiment in the response\n",
    "        \"includeEventDate\": True,              # Include event date in the response\n",
    "        \"includeEventArticleCounts\": True,     # Include article counts to measure coverage\n",
    "        \"includeEventRelevanceScore\": True,\n",
    "    },}\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and store results\n",
    "events = []\n",
    "for event in q.execQuery(er,returnInfo=return_info, maxItems=100):  # added returnInfo\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    social_score = event.get(\"socialScore\", 1)  # Ensure this field exists and avoid division by zero\n",
    "    if article_count > 0:  # Avoid division by zero\n",
    "        ratio = social_score / article_count  # Ratio of social score to article count\n",
    "        events.append({\n",
    "            \"date\": event_date,\n",
    "            \"article_count\": article_count,\n",
    "            \"social_score\": social_score,\n",
    "            \"ratio\": ratio\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "# Sort events based on the ratio of article count to social score\n",
    "social_events_immigration = sorted(events, key=lambda x: x[\"ratio\"], reverse=True)\n",
    "\n",
    "#might delete later\n",
    "# Print events with the highest and lowest ratios\n",
    "print(\"\\nImmigration events with high media coverage (Social Score / Article Count):\")\n",
    "for event in social_events_immigration[:5]:\n",
    "    print(f\"Date: {event['date']}, Social Score: {event['social_score']}, Article Count: {event['article_count']}, Ratio: {event['ratio']}\")\n",
    "\n",
    "print(\"Immigration events with low media coverage (Social Score / Article Count):\")\n",
    "for event in social_events_immigration[-5:]:\n",
    "    print(f\"Date: {event['date']}, Social Score: {event['social_score']}, Article Count: {event['article_count']}, Ratio: {event['ratio']}\")\n",
    "\n",
    "\n",
    "\n",
    "#war on Ukraine\n",
    "#\n",
    "# Query parameters for events related to the war on Ukraine with correct filters\n",
    "query = {\n",
    "    \"$query\": {\n",
    "        \"$and\": [\n",
    "            {\"keyword\": \"war on Ukraine\"},                                   # Already very specific. No room for misunderstanding\n",
    "            {\"categoryUri\": \"news/Politics\"},\n",
    "            {\n",
    "                \"dateStart\": \"2022-11-08\",\n",
    "                \"dateEnd\": \"2024-07-18\",\n",
    "                \"minArticlesInEvent\": min_articles,\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "    \"$filter\": {\n",
    "        \"minRelevanceScore\": min_relevance,         # Set minimum relevance score\n",
    "        \"includeEventSentiment\": True,         # Include sentiment in the response\n",
    "        \"includeEventDate\": True,              # Include event date in the response\n",
    "        \"includeEventArticleCounts\": True,     # Include article counts to measure coverage\n",
    "        \"includeEventRelevanceScore\": True,\n",
    "    },}\n",
    "\n",
    "# Initialize the query for events\n",
    "q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "# Execute the query and store results\n",
    "events = []\n",
    "for event in q.execQuery(er,returnInfo=return_info, maxItems=100):  # added returnInfo\n",
    "    event_date = event.get(\"eventDate\", \"No Date\")\n",
    "    article_count = event.get(\"totalArticleCount\", 0)\n",
    "    social_score = event.get(\"socialScore\", 1)  # Ensure this field exists and avoid division by zero\n",
    "    if article_count > 0:  # Avoid division by zero\n",
    "        ratio = social_score / article_count  # Ratio of social score to article count\n",
    "        events.append({\n",
    "            \"date\": event_date,\n",
    "            \"article_count\": article_count,\n",
    "            \"social_score\": social_score,\n",
    "            \"ratio\": ratio\n",
    "        })\n",
    "\n",
    "\n",
    "# Sort events based on the ratio of article count to social score\n",
    "social_events_Ukraine = sorted(events, key=lambda x: x[\"ratio\"], reverse=True)\n",
    "\n",
    "#felt cute, might delete later\n",
    "# Print events with the highest and lowest ratios\n",
    "print(\"\\nUkraine events with high media coverage (Social Score / Article Count):\")\n",
    "for event in social_events_Ukraine[:5]:\n",
    "    print(f\"Date: {event['date']}, Social Score: {event['social_score']}, Article Count: {event['article_count']}, Ratio: {event['ratio']}\")\n",
    "\n",
    "print(\"Ukraine events with low media coverage (Social Score / Article Count):\")\n",
    "for event in social_events_Ukraine[-5:]:\n",
    "    print(f\"Date: {event['date']}, Social Score: {event['social_score']}, Article Count: {event['article_count']}, Ratio: {event['ratio']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-impact event search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking other events that may have influenced our target periods.\n",
    "In this section, we track events occurring during the same time frame as our target event, where we are studying changes in political orientation. These events could have a significant impact on shifts in political orientation (such as the attempted assassination of Trump) but fall outside our primary areas of investigation. Identifying these events is crucial to avoid mistakenly attributing observed changes in political orientation to our primary event, when they might instead be influenced by these other high-impact occurrences.\n",
    "This method takes a list (e.g., big_events_energy) as input. It reads the date and article count of each event in the list. Next, it determines a two-week interval centered around the date of our target event. Using the EventRegistry, it then searches for other events occurring within this interval that have a higher article count. Events for which no higher article count is found within this period are added to a new list, solo_big_events_energy, and are printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventregistry import *\n",
    "import datetime\n",
    "\n",
    "# Initialize EventRegistry with your API key\n",
    "er = EventRegistry(apiKey=\"be75941d-07ae-406a-ad2d-9d3cfe44c892\")\n",
    "\n",
    "def find_solo_events(event_list):\n",
    "    solo_events = []\n",
    "\n",
    "    # Iterate through each event in the provided list\n",
    "    for event in event_list:\n",
    "        target_event_date = datetime.datetime.strptime(event['date'], '%Y-%m-%d')  # Adjust date format as necessary\n",
    "        target_article_count = event['article_count']\n",
    "\n",
    "        # Define a two-week interval centered around the target event's date\n",
    "        start_date = (target_event_date - datetime.timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "        end_date = (target_event_date + datetime.timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "\n",
    "        # Define a complex query for events within the date range and with a minimum article count\n",
    "        query = {\n",
    "            \"$query\": {\n",
    "                \"$and\": [\n",
    "                    {\"dateStart\": start_date, \"dateEnd\": end_date},\n",
    "                    {\"minArticlesInEvent\": target_article_count + 1}  # Only look for events with a higher article count\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Initialize the query for events using the complex query structure\n",
    "        q = QueryEventsIter.initWithComplexQuery(query)\n",
    "\n",
    "        # Execute the query to get events within the interval\n",
    "        found_higher_count = False\n",
    "        for other_event in q.execQuery(er, maxItems=100):  # Limit to 100 events\n",
    "            # Skip if we are comparing the event with itself\n",
    "            if other_event['uri'] == event['uri']:\n",
    "                continue\n",
    "\n",
    "            # Get article count for the other event\n",
    "            other_event_article_count = other_event.get('totalArticleCount', 0)\n",
    "\n",
    "            # Check if there's an event with a higher article count\n",
    "            if other_event_article_count > target_article_count:\n",
    "                found_higher_count = True\n",
    "                print (other_event_article_count)\n",
    "                break\n",
    "\n",
    "        # If no higher-count event was found in the interval, add it to solo_events\n",
    "        if not found_higher_count:\n",
    "            solo_events.append(event)\n",
    "    \"\"\"\n",
    "    # Print or return the list of solo events\n",
    "    print(\"Solo Big Events:\")\n",
    "    for solo_event in solo_events:\n",
    "        print(solo_event)\n",
    "    \"\"\"\n",
    "    return solo_events\n",
    "\n",
    "# Example usage:\n",
    "# Assuming big_events_energy is a list of dictionaries with each event's 'date', 'uri', and 'article_count'\n",
    "big_events_energy = [\n",
    "    {\"uri\": \"eng-123456\", \"date\": \"2024-10-15\", \"article_count\": 3000},\n",
    "    {\"uri\": \"eng-789101\", \"date\": \"2024-10-18\", \"article_count\": 8000},\n",
    "    {\"uri\": \"eng-789101\", \"date\": \"2024-10-18\", \"article_count\": 8000},\n",
    "    {\"uri\": \"eng-789101\", \"date\": \"2024-10-18\", \"article_count\": 8000},\n",
    "    {\"uri\": \"eng-789101\", \"date\": \"2024-10-18\", \"article_count\": 8000},\n",
    "    {\"uri\": \"eng-789101\", \"date\": \"2024-10-18\", \"article_count\": 8000},\n",
    "    {\"uri\": \"eng-789101\", \"date\": \"2024-10-18\", \"article_count\": 8000},\n",
    "]\n",
    "\n",
    "solo_big_events_energy = find_solo_events(big_events_energy)\n",
    "print(solo_big_events_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this Code expects a string event_uri\n",
    "# eng-9621683, 2024-06-04, Biden order to block most illegal immigrants when crossings surge, as election nears\n",
    "\n",
    "from eventregistry import *\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# Initialize EventRegistry with API key\n",
    "er = EventRegistry(apiKey=\"be75941d-07ae-406a-ad2d-9d3cfe44c892\")\n",
    "\n",
    "# Query articles for the specified event URI\n",
    "iter = QueryEventArticlesIter(\"eng-9621683\", lang=\"eng\")\n",
    "\n",
    "# Store publication times\n",
    "times = []\n",
    "\n",
    "i = 0\n",
    "# Collect dateTime for articles\n",
    "for art in iter.execQuery(er, sortBy=\"date\"):\n",
    "    if i > 500:  # To limit the token usage, break after 500 articles\n",
    "        break\n",
    "    else:\n",
    "        date_time_str = art[\"dateTime\"]\n",
    "        # Adjust the format to handle 'Z' for UTC timezone\n",
    "        date_time_obj = datetime.datetime.strptime(date_time_str, '%Y-%m-%dT%H:%M:%SZ')\n",
    "        times.append(date_time_obj)\n",
    "        i += 1\n",
    "\n",
    "# Sort the times for correct plotting\n",
    "times.sort()\n",
    "\n",
    "# Plotting the article publication timeline\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(times, bins=50, color='blue', edgecolor='black')  # Histogram for frequency of articles over time\n",
    "plt.xlabel('Publication Date')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Article Publication Timeline for Event eng-9621683')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chow Test and local gradient analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevanter Klassen / Module / Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.stats import f, linregress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chow Test and local gradient calculation for hypothesis 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Lade das Datenset \n",
    "file_path = '/content/data_sample_700_SOSEC_dataset_germany_inTabellenform.csv'\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1', sep=';', on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "# 2. Konvertiere die Spalte 'i_START' in ein Datumsformat\n",
    "data['i_START'] = pd.to_datetime(data['i_START'], errors='coerce')\n",
    "\n",
    "# 3. Notiere, welche Daten fehlen\n",
    "missing_data = data.isna()\n",
    "print(\"Fehlende Werte pro Spalte:\")\n",
    "print(missing_data.sum())\n",
    "\n",
    "# 4. Umbenennen der Spalten fÃ¼r bessere Lesbarkeit \n",
    "data.rename(columns={\n",
    "    'i_START': 'date',\n",
    "    'F6mA1_1': 'self_assessment',\n",
    "    'F6a_linkeA1': 'linke_support',\n",
    "    'F6a_afdA1': 'afd_support',\n",
    "    'F6a_bswA1': 'bsw_support'\n",
    "}, inplace=True)\n",
    "\n",
    "# 5. Neue Tabelle erstellen mit den gewÃ¼nschten Spalten\n",
    "new_data = data.loc[:, ['date', 'bsw_support', 'self_assessment', 'afd_support', 'linke_support']]\n",
    "\n",
    "\n",
    "# 6. Notiere die fehlenden Daten fÃ¼r die neuen Spalten\n",
    "missing_new_data = new_data.isna()\n",
    "print(\"Fehlende Werte in den ausgewÃ¤hlten Spalten:\")\n",
    "print(missing_new_data.sum())\n",
    "\n",
    "# 7. Neue Tabelle in eine CSV-Datei exportieren\n",
    "new_data.to_csv('/content/bereinigte_umfragedaten.csv', index=False)\n",
    "\n",
    "# 8. Die neue Tabelle anzeigen, um sicherzustellen, dass alles korrekt ist\n",
    "print(new_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to perform Chow test\n",
    "def chow_test(y, X, split_index):\n",
    "    model_full = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    X1, y1 = X[:split_index], y[:split_index]\n",
    "    X2, y2 = X[split_index:], y[split_index:]\n",
    "    model_1 = sm.OLS(y1, sm.add_constant(X1)).fit()\n",
    "    model_2 = sm.OLS(y2, sm.add_constant(X2)).fit()\n",
    "\n",
    "    rss_full = model_full.ssr\n",
    "    rss_1, rss_2 = model_1.ssr, model_2.ssr\n",
    "    k = X.shape[1] + 1\n",
    "    n1, n2 = len(y1), len(y2)\n",
    "\n",
    "    chow_stat = ((rss_full - (rss_1 + rss_2)) / k) / ((rss_1 + rss_2) / (n1 + n2 - 2 * k))\n",
    "    p_value = 1 - f.cdf(chow_stat, k, n1 + n2 - 2 * k)\n",
    "\n",
    "    return chow_stat, p_value, model_1, model_2\n",
    "\n",
    "# Function to perform Chow tests\n",
    "def perform_chow_tests(response_variable, event_dates, survey_data, hypothesis, event_type, window_size=60):\n",
    "    results = []\n",
    "    for event_date in event_dates:\n",
    "        start_date = event_date - timedelta(days=window_size)\n",
    "        end_date = event_date + timedelta(days=window_size)\n",
    "        window_data = survey_data[(survey_data['date'] >= start_date) & (survey_data['date'] <= end_date)]\n",
    "\n",
    "        y = window_data[response_variable]\n",
    "        X = window_data['date'].map(datetime.toordinal)  # Use time (date as ordinal) as predictor\n",
    "        X = X.values.reshape(-1, 1)  # Reshape to 2D array for regression\n",
    "        split_index = len(window_data[window_data['date'] < event_date])\n",
    "\n",
    "        if len(y) < 2 or split_index == 0 or split_index == len(y):\n",
    "            chow_stat, p_value, change_magnitude, coverage = np.nan, np.nan, np.nan, np.nan\n",
    "        else:\n",
    "            chow_stat, p_value, model_1, model_2 = chow_test(y, X, split_index)\n",
    "            change_magnitude = abs(model_2.params[1] - model_1.params[1])  # Difference in slopes as a measure of change\n",
    "            coverage = linregress(X.flatten(), y).slope  # Local gradient over the full window\n",
    "\n",
    "        results.append({\n",
    "            'event_date': event_date.strftime('%Y-%m-%d'),\n",
    "            'hypothesis': hypothesis,\n",
    "            'Event': event_type,\n",
    "            'response_variable': response_variable,\n",
    "            'chow_stat': chow_stat,\n",
    "            'p_value': p_value,\n",
    "            'change_magnitude': change_magnitude,\n",
    "            'significant_change': p_value < 0.05,\n",
    "            'local_gradient': coverage\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load the survey data\n",
    "file_path = '/content/bereinigte_umfragedaten.csv'  \n",
    "survey_data = pd.read_csv(file_path)\n",
    "survey_data['date'] = pd.to_datetime(survey_data['date'])\n",
    "survey_data_clean = survey_data.dropna(subset=['afd_support', 'linke_support'])\n",
    "\n",
    "# Define event dates grouped by hypothesis and event type\n",
    "event_data = {\n",
    "    \"H1\": {\n",
    "        \"energy crisis\": [datetime(2024, 1, 25), datetime(2023, 2, 7), datetime(2023, 10, 24), datetime(2023, 1, 12), datetime(2022, 12, 19)],\n",
    "        \"rising immigration\": [datetime(2024, 6, 4), datetime(2023, 10, 5), datetime(2023, 5, 23)],\n",
    "        \"war on ukraine\": [datetime(2024, 7, 16), datetime(2024, 7, 17), datetime(2024, 7, 18)]\n",
    "    },\n",
    "    \"H2\": {\n",
    "\n",
    "    \"energy_high_coverage\": [\n",
    "        datetime(2023, 8, 21),\n",
    "        datetime(2023, 2, 9),\n",
    "        datetime(2023, 2, 17),\n",
    "        datetime(2023, 1, 17),\n",
    "        datetime(2023, 8, 4)\n",
    "    ],\n",
    "    \"energy_low_coverage\": [\n",
    "        datetime(2022, 11, 11),\n",
    "        datetime(2023, 3, 13),\n",
    "        datetime(2022, 11, 24),\n",
    "        datetime(2023, 2, 17),\n",
    "        datetime(2022, 11, 22)\n",
    "    ],\n",
    "    \"immigration_high_coverage\": [\n",
    "        datetime(2023, 5, 16),\n",
    "        datetime(2023, 9, 15),\n",
    "        datetime(2023, 4, 10),\n",
    "        datetime(2023, 8, 30),\n",
    "        datetime(2024, 2, 19)\n",
    "    ],\n",
    "    \"immigration_low_coverage\": [\n",
    "        datetime(2023, 11, 23),\n",
    "        datetime(2023, 10, 30),\n",
    "        datetime(2024, 4, 3),\n",
    "        datetime(2023, 10, 20),\n",
    "        datetime(2023, 4, 19)\n",
    "    ],\n",
    "    \"ukraine_high_coverage\": [\n",
    "        datetime(2024, 7, 6),\n",
    "        datetime(2024, 7, 4),\n",
    "        datetime(2024, 7, 15),\n",
    "        datetime(2024, 7, 7),\n",
    "        datetime(2024, 7, 18)\n",
    "    ],\n",
    "    \"ukraine_low_coverage\": [\n",
    "        datetime(2023, 9, 19),\n",
    "        datetime(2022, 11, 17),\n",
    "        datetime(2023, 2, 20),\n",
    "        datetime(2024, 5, 5),\n",
    "        datetime(2023, 2, 20)\n",
    "    ]\n",
    "\n",
    "\n",
    "},\n",
    "\n",
    "    \"H3\": {\n",
    "        \"energy crisis\": [datetime(2022, 12, 28), datetime(2022, 12, 16), datetime(2023, 8, 6), datetime(2022, 12, 6), datetime(2022, 11, 22)],\n",
    "        \"rising immigration\": [datetime(2024, 6, 4), datetime(2023, 5, 11), datetime(2023, 3, 28), datetime(2024, 2, 26), datetime(2024, 6, 18)],\n",
    "        \"war on ukraine\": [datetime(2023, 6, 24), datetime(2023, 3, 18), datetime(2024, 6, 18), datetime(2023, 6, 26), datetime(2023, 2, 20)]\n",
    "    },\n",
    "    \"H4\": {\n",
    "        \"energy crisis\": [datetime(2023, 8, 21), datetime(2023, 2, 9), datetime(2023, 2, 17), datetime(2023, 1, 17), datetime(2023, 8, 4)],\n",
    "        \"rising immigration\": [datetime(2023, 9, 15), datetime(2023, 4, 10), datetime(2023, 8, 30), datetime(2024, 2, 19), datetime(2023, 11, 23)],\n",
    "        \"war on ukraine\": [datetime(2024, 7, 6), datetime(2024, 7, 4), datetime(2024, 7, 15), datetime(2024, 7, 7), datetime(2024, 7, 18)]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare final results\n",
    "chow_results = []\n",
    "\n",
    "for hypothesis, events in event_data.items():\n",
    "    for event_type, dates in events.items():\n",
    "        chow_results.append(perform_chow_tests('afd_support', dates, survey_data_clean, hypothesis, event_type))\n",
    "        chow_results.append(perform_chow_tests('linke_support', dates, survey_data_clean, hypothesis, event_type))\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "final_results = pd.concat(chow_results, ignore_index=True)\n",
    "\n",
    "# Save to Excel\n",
    "output_file = 'final_results.xlsx'\n",
    "final_results.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustergruppen \n",
    "\n",
    "1. Impact groups energy crisis\n",
    "2. Impact groups immigration crisis\n",
    "3. Impact groups ukraine war\n",
    "4. Groups according to extreme political attitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Impact groups energy crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/content/data_sample_700_SOSEC_dataset_germany_inTabellenform.csv'  # Replace with your actual file path\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1', sep=';', on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "# Ensure relevant columns are numeric and replace invalid values\n",
    "relevant_columns = ['einkommen', 'F2A1', 'F1A15_1', 'F1A13_1', 'F1A14_1']\n",
    "for col in relevant_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce').replace(99, pd.NA)\n",
    "\n",
    "# Convert 'i_START' to datetime\n",
    "data['date'] = pd.to_datetime(data['i_START'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "data.rename(columns={\n",
    "    'F6a_afdA1': 'afd_support',\n",
    "    'F6a_linkeA1': 'linke_support'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the function to classify high-impact individuals including new questions\n",
    "def classify_high_impact(row):\n",
    "    financial = (pd.notna(row['einkommen']) and row['einkommen'] <= 3) or (row['F2A1'] >= 4)  # Financial strain\n",
    "    emotional = row['F1A15_1'] >= 4  # Emotional agreement\n",
    "    perception = row['F1A13_1'] <= 3  # Strong agreement with \"Energiekrise verschlechtert meine Situation\"\n",
    "    coping = row['F1A14_1'] >= 5  # Strong disagreement with \"Ich kann einfacher umgehen\"\n",
    "\n",
    "    high_impact_reasons = []\n",
    "    if financial:\n",
    "        high_impact_reasons.append('financial')\n",
    "    if emotional:\n",
    "        high_impact_reasons.append('emotional')\n",
    "    if perception:\n",
    "        high_impact_reasons.append('perception')\n",
    "    if coping:\n",
    "        high_impact_reasons.append('coping')\n",
    "\n",
    "    return ', '.join(high_impact_reasons) if high_impact_reasons else None\n",
    "\n",
    "# Apply the classification to the dataset\n",
    "data['high_impact_reasons'] = data.apply(classify_high_impact, axis=1)\n",
    "\n",
    "# Filter dataset to include only high-impact individuals\n",
    "high_impact_data = data[data['high_impact_reasons'].notnull()]\n",
    "\n",
    "# Select relevant columns for the new dataset\n",
    "output_columns = ['unique_id', 'date', 'afd_support', 'linke_support', 'high_impact_reasons']\n",
    "high_impact_dataset = high_impact_data[output_columns]\n",
    "\n",
    "# Save the new dataset to a file\n",
    "output_high_impact_file = 'high_impact_energy_crisis_values.xlsx'\n",
    "high_impact_dataset.to_excel(output_high_impact_file, index=False)\n",
    "\n",
    "print(f\"High-impact dataset saved to {output_high_impact_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chow test and local gradients for energy crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to perform Chow test\n",
    "def chow_test(y, X, split_index):\n",
    "    model_full = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    X1, y1 = X[:split_index], y[:split_index]\n",
    "    X2, y2 = X[split_index:], y[split_index:]\n",
    "    model_1 = sm.OLS(y1, sm.add_constant(X1)).fit()\n",
    "    model_2 = sm.OLS(y2, sm.add_constant(X2)).fit()\n",
    "\n",
    "    rss_full = model_full.ssr\n",
    "    rss_1, rss_2 = model_1.ssr, model_2.ssr\n",
    "    k = X.shape[1] + 1\n",
    "    n1, n2 = len(y1), len(y2)\n",
    "\n",
    "    chow_stat = ((rss_full - (rss_1 + rss_2)) / k) / ((rss_1 + rss_2) / (n1 + n2 - 2 * k))\n",
    "    p_value = 1 - f.cdf(chow_stat, k, n1 + n2 - 2 * k)\n",
    "\n",
    "    return chow_stat, p_value, model_1, model_2\n",
    "\n",
    "# Function to perform Chow tests\n",
    "def perform_chow_tests(response_variable, event_dates, survey_data, group_name, hypothesis, event_type, window_size=60):\n",
    "    results = []\n",
    "    for event_date in event_dates:\n",
    "        start_date = event_date - timedelta(days=window_size)\n",
    "        end_date = event_date + timedelta(days=window_size)\n",
    "        window_data = survey_data[(survey_data['date'] >= start_date) & (survey_data['date'] <= end_date)]\n",
    "\n",
    "        y = window_data[response_variable]\n",
    "        X = window_data['date'].map(datetime.toordinal)  # Use time (date as ordinal) as predictor\n",
    "        X = X.values.reshape(-1, 1)  # Reshape to 2D array for regression\n",
    "        split_index = len(window_data[window_data['date'] < event_date])\n",
    "\n",
    "        if len(y) < 2 or split_index == 0 or split_index == len(y):\n",
    "            chow_stat, p_value, change_magnitude, coverage = np.nan, np.nan, np.nan, np.nan\n",
    "        else:\n",
    "            chow_stat, p_value, model_1, model_2 = chow_test(y, X, split_index)\n",
    "            change_magnitude = abs(model_2.params.iloc[1] - model_1.params.iloc[1])  # Difference in slopes as a measure of change\n",
    "            coverage = linregress(X.flatten(), y).slope  # Local gradient over the full window\n",
    "\n",
    "        results.append({\n",
    "            'group': group_name,\n",
    "            'event_date': event_date.strftime('%Y-%m-%d'),\n",
    "            'hypothesis': hypothesis,\n",
    "            'Event': event_type,\n",
    "            'response_variable': response_variable,\n",
    "            'chow_stat': chow_stat,\n",
    "            'p_value': p_value,\n",
    "            'change_magnitude': change_magnitude,\n",
    "            'significant_change': p_value < 0.05,\n",
    "            'local_gradient': coverage\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/content/high_impact_dataset_with_new_questions.xlsx'  \n",
    "survey_data = pd.read_excel(file_path)\n",
    "survey_data['date'] = pd.to_datetime(survey_data['date'])\n",
    "survey_data_clean = survey_data.dropna(subset=['afd_support', 'linke_support', 'high_impact_reasons'])\n",
    "\n",
    "# Define event dates grouped by hypothesis and event type\n",
    "event_dates = [\n",
    "    datetime(2024, 1, 25), datetime(2023, 2, 7), datetime(2023, 10, 24), datetime(2023, 1, 12), datetime(2022, 12, 19)\n",
    "]\n",
    "\n",
    "# Initialize list for grouped and overall results\n",
    "grouped_results = []\n",
    "\n",
    "# Perform Chow tests for grouped data based on high-impact reasons\n",
    "for category in ['financial', 'emotional', 'perception', 'coping']:\n",
    "    subset = survey_data_clean[survey_data_clean['high_impact_reasons'].str.contains(category)]\n",
    "    if not subset.empty:\n",
    "        afd_results = perform_chow_tests('afd_support', event_dates, subset, category, 'H8', 'energy crisis')\n",
    "        linke_results = perform_chow_tests('linke_support', event_dates, subset, category, 'H8', 'energy crisis')\n",
    "        grouped_results.append(afd_results)\n",
    "        grouped_results.append(linke_results)\n",
    "\n",
    "# Perform Chow tests for overall data (without grouping)\n",
    "overall_afd_results = perform_chow_tests('afd_support', event_dates, survey_data_clean, 'overall', 'H8', 'energy crisis')\n",
    "overall_linke_results = perform_chow_tests('linke_support', event_dates, survey_data_clean, 'overall', 'H8', 'energy crisis')\n",
    "\n",
    "# Add overall results to the grouped results\n",
    "grouped_results.append(overall_afd_results)\n",
    "grouped_results.append(overall_linke_results)\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "final_results = pd.concat(grouped_results, ignore_index=True)\n",
    "\n",
    "# Save to Excel\n",
    "output_file = 'final_results_energy_crisis.xlsx'\n",
    "final_results.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Impact groups immigration crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "file_path = '/content/data_sample_700_SOSEC_dataset_germany_inTabellenform.csv'  \n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1', sep=';', on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "# Ensure relevant columns are numeric and replace invalid values\n",
    "relevant_columns = ['einkommen', 'F2A8', 'F2A7', 'F1A9_1', 'F3A7_1', 'F3A8_1', 'F6a_afdA1', 'F6a_linkeA1']\n",
    "for col in relevant_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce').replace(99, pd.NA)\n",
    "\n",
    "# Convert 'i_START' to datetime\n",
    "data['date'] = pd.to_datetime(data['i_START'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "data.rename(columns={\n",
    "    'F6a_afdA1': 'afd_support',\n",
    "    'F6a_linkeA1': 'linke_support'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the function to classify high-impact individuals for immigration crisis\n",
    "def classify_immigration_impact(row):\n",
    "    # Financial strain: einkommen â‰¤ 3\n",
    "    financial = (pd.notna(row['einkommen']) and row['einkommen'] <= 3)\n",
    "\n",
    "    # High concerns about refugees: F2A8 â‰¥ 4 (non-Ukrainian) or F2A7 â‰¥ 4 (Ukrainian)\n",
    "    concerns = (row['F2A8'] >= 4) or (row['F2A7'] >= 4)\n",
    "\n",
    "    # Anger and negative opinions about immigration:\n",
    "    anger = row['F1A9_1'] <= 3  # Strong anger about immigration\n",
    "    crime_opinion = row['F3A7_1'] <= 3  # Agreement with increased crime due to foreigners\n",
    "    job_threat_opinion = row['F3A8_1'] <= 3  # Agreement with job threat due to foreigners\n",
    "\n",
    "    # Collect reasons for high impact\n",
    "    high_impact_reasons = []\n",
    "    if financial:\n",
    "        high_impact_reasons.append('financial')\n",
    "    if concerns:\n",
    "        high_impact_reasons.append('concerns')\n",
    "    if anger:\n",
    "        high_impact_reasons.append('anger')\n",
    "    if crime_opinion:\n",
    "        high_impact_reasons.append('crime_opinion')\n",
    "    if job_threat_opinion:\n",
    "        high_impact_reasons.append('job_threat_opinion')\n",
    "\n",
    "    return ', '.join(high_impact_reasons) if high_impact_reasons else None\n",
    "\n",
    "# Apply the classification to the dataset\n",
    "data['immigration_high_impact_reasons'] = data.apply(classify_immigration_impact, axis=1)\n",
    "\n",
    "# Filter dataset to include only high-impact individuals\n",
    "immigration_high_impact_data = data[data['immigration_high_impact_reasons'].notnull()]\n",
    "\n",
    "# Select relevant columns for the new dataset\n",
    "output_columns = ['unique_id', 'date', 'afd_support', 'linke_support', 'immigration_high_impact_reasons']\n",
    "high_impact_dataset = immigration_high_impact_data[output_columns]\n",
    "\n",
    "# Save the new dataset to a file\n",
    "output_high_impact_file = 'high_impact_immigration_crisis_values.xlsx'\n",
    "high_impact_dataset.to_excel(output_high_impact_file, index=False)\n",
    "\n",
    "print(f\"High-impact immigration dataset saved to {output_high_impact_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Impact groups Ukraine war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "file_path = '/content/data_sample_700_SOSEC_dataset_germany_inTabellenform.csv'  \n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1', sep=';', on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "# Ensure relevant columns are numeric and replace invalid values\n",
    "relevant_columns = ['einkommen', 'F2A7', 'F2A8', 'F3A7_1', 'F3A8_1', 'F6a_afdA1', 'F6a_linkeA1']\n",
    "for col in relevant_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce').replace(99, pd.NA)\n",
    "\n",
    "# Convert 'i_START' to datetime\n",
    "data['date'] = pd.to_datetime(data['i_START'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "data.rename(columns={\n",
    "    'F6a_afdA1': 'afd_support',\n",
    "    'F6a_linkeA1': 'linke_support'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the function to classify high-impact individuals for the Ukraine war\n",
    "def classify_ukraine_impact(row):\n",
    "    # Financial strain: einkommen â‰¤ 3\n",
    "    financial = (pd.notna(row['einkommen']) and row['einkommen'] <= 3)\n",
    "\n",
    "    # High concerns about refugees: F2A7 (Ukrainian) â‰¥ 4 or F2A8 (non-Ukrainian) â‰¥ 4\n",
    "    concerns = (row['F2A7'] >= 4) or (row['F2A8'] >= 4)\n",
    "\n",
    "    # Strong agreement with immigration-related problems: F3A7_1 â‰¤ 3 (crime), F3A8_1 â‰¤ 3 (job threats)\n",
    "    crime_opinion = row['F3A7_1'] <= 3\n",
    "    job_threat_opinion = row['F3A8_1'] <= 3\n",
    "\n",
    "    # Collect reasons for high impact\n",
    "    high_impact_reasons = []\n",
    "    if financial:\n",
    "        high_impact_reasons.append('financial')\n",
    "    if concerns:\n",
    "        high_impact_reasons.append('concerns')\n",
    "    if crime_opinion:\n",
    "        high_impact_reasons.append('crime_opinion')\n",
    "    if job_threat_opinion:\n",
    "        high_impact_reasons.append('job_threat_opinion')\n",
    "\n",
    "    return ', '.join(high_impact_reasons) if high_impact_reasons else None\n",
    "\n",
    "# Apply the classification to the dataset\n",
    "data['ukraine_high_impact_reasons'] = data.apply(classify_ukraine_impact, axis=1)\n",
    "\n",
    "# Filter dataset to include only high-impact individuals\n",
    "ukraine_high_impact_data = data[data['ukraine_high_impact_reasons'].notnull()]\n",
    "\n",
    "# Select relevant columns for the new dataset\n",
    "output_columns = ['unique_id', 'date', 'afd_support', 'linke_support', 'ukraine_high_impact_reasons']\n",
    "high_impact_dataset = ukraine_high_impact_data[output_columns]\n",
    "\n",
    "# Save the new dataset to a file\n",
    "output_high_impact_file = 'high_impact_ukraine_war_values.xlsx'\n",
    "high_impact_dataset.to_excel(output_high_impact_file, index=False)\n",
    "\n",
    "print(f\"High-impact Ukraine war dataset saved to {output_high_impact_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chow Test and local gradients for immigration crisis and Ukraine war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to perform Chow test\n",
    "def chow_test(y, X, split_index):\n",
    "    model_full = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    X1, y1 = X[:split_index], y[:split_index]\n",
    "    X2, y2 = X[split_index:], y[split_index:]\n",
    "    model_1 = sm.OLS(y1, sm.add_constant(X1)).fit()\n",
    "    model_2 = sm.OLS(y2, sm.add_constant(X2)).fit()\n",
    "\n",
    "    rss_full = model_full.ssr\n",
    "    rss_1, rss_2 = model_1.ssr, model_2.ssr\n",
    "    k = X.shape[1] + 1\n",
    "    n1, n2 = len(y1), len(y2)\n",
    "\n",
    "    chow_stat = ((rss_full - (rss_1 + rss_2)) / k) / ((rss_1 + rss_2) / (n1 + n2 - 2 * k))\n",
    "    p_value = 1 - f.cdf(chow_stat, k, n1 + n2 - 2 * k)\n",
    "\n",
    "    return chow_stat, p_value, model_1, model_2\n",
    "\n",
    "# Function to perform Chow tests\n",
    "def perform_chow_tests(response_variable, event_dates, survey_data, group_name, hypothesis, event_type, window_size=60):\n",
    "    results = []\n",
    "    for event_date in event_dates:\n",
    "        start_date = event_date - timedelta(days=window_size)\n",
    "        end_date = event_date + timedelta(days=window_size)\n",
    "        window_data = survey_data[(survey_data['date'] >= start_date) & (survey_data['date'] <= end_date)]\n",
    "\n",
    "        y = window_data[response_variable]\n",
    "        X = window_data['date'].map(datetime.toordinal)  # Use time (date as ordinal) as predictor\n",
    "        X = X.values.reshape(-1, 1)  # Reshape to 2D array for regression\n",
    "        split_index = len(window_data[window_data['date'] < event_date])\n",
    "\n",
    "        if len(y) < 2 or split_index == 0 or split_index == len(y):\n",
    "            chow_stat, p_value, change_magnitude, coverage = np.nan, np.nan, np.nan, np.nan\n",
    "        else:\n",
    "            chow_stat, p_value, model_1, model_2 = chow_test(y, X, split_index)\n",
    "            change_magnitude = abs(model_2.params.iloc[1] - model_1.params.iloc[1])  # Difference in slopes as a measure of change\n",
    "            coverage = linregress(X.flatten(), y).slope  # Local gradient over the full window\n",
    "\n",
    "        results.append({\n",
    "            'group': group_name,\n",
    "            'event_date': event_date.strftime('%Y-%m-%d'),\n",
    "            'hypothesis': hypothesis,\n",
    "            'Event': event_type,\n",
    "            'response_variable': response_variable,\n",
    "            'chow_stat': chow_stat,\n",
    "            'p_value': p_value,\n",
    "            'change_magnitude': change_magnitude,\n",
    "            'significant_change': p_value < 0.05,\n",
    "            'local_gradient': coverage\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load the high-impact immigration dataset\n",
    "file_path = '/content/high_impact_immigration_crisis_values.xlsx'  # Replace with your actual file path\n",
    "survey_data = pd.read_excel(file_path)\n",
    "survey_data['date'] = pd.to_datetime(survey_data['date'])\n",
    "survey_data_clean = survey_data.dropna(subset=['afd_support', 'linke_support', 'immigration_high_impact_reasons'])\n",
    "\n",
    "# Define event dates for the immigration crisis\n",
    "event_dates = [\n",
    "    datetime(2024, 6, 4), datetime(2023, 10, 5), datetime(2023, 5, 23),\n",
    "    datetime(2024, 7, 16), datetime(2024, 7, 17), datetime(2024, 7, 18)\n",
    "]\n",
    "\n",
    "# Define the grouping categories\n",
    "categories = ['financial', 'concerns', 'anger', 'crime_opinion', 'job_threat_opinion']\n",
    "\n",
    "# Perform Chow tests for each category\n",
    "grouped_results = []\n",
    "\n",
    "for category in categories:\n",
    "    subset = survey_data_clean[survey_data_clean['immigration_high_impact_reasons'].str.contains(category)]\n",
    "    if not subset.empty:\n",
    "        afd_results = perform_chow_tests('afd_support', event_dates, subset, category, 'H8', 'immigration crisis')\n",
    "        linke_results = perform_chow_tests('linke_support', event_dates, subset, category, 'H8', 'immigration crisis')\n",
    "        grouped_results.append(afd_results)\n",
    "        grouped_results.append(linke_results)\n",
    "\n",
    "# Perform Chow tests for the overall dataset\n",
    "overall_afd_results = perform_chow_tests('afd_support', event_dates, survey_data_clean, 'overall', 'H8', 'immigration crisis')\n",
    "overall_linke_results = perform_chow_tests('linke_support', event_dates, survey_data_clean, 'overall', 'H8', 'immigration crisis')\n",
    "\n",
    "# Add overall results to the grouped results\n",
    "grouped_results.append(overall_afd_results)\n",
    "grouped_results.append(overall_linke_results)\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "final_results = pd.concat(grouped_results, ignore_index=True)\n",
    "\n",
    "# Save to Excel\n",
    "output_file = 'final_results_with_overall_and_immigration_grouped.xlsx'\n",
    "final_results.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Groups according to extreme political attitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "file_path = '/content/data_sample_700_SOSEC_dataset_germany_inTabellenform.csv'  \n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1', sep=';', on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "# Ensure relevant columns are numeric and replace invalid values\n",
    "relevant_columns = ['F6a_afdA1', 'F6a_linkeA1']\n",
    "for col in relevant_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce').replace(99, pd.NA)\n",
    "\n",
    "# Convert 'i_START' to datetime\n",
    "data['date'] = pd.to_datetime(data['i_START'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Calculate the top 10% thresholds for AfD and Die Linke scores\n",
    "afd_threshold = data['F6a_afdA1'].quantile(0.9)\n",
    "linke_threshold = data['F6a_linkeA1'].quantile(0.9)\n",
    "\n",
    "print(f\"Top 10% threshold for AfD support: {afd_threshold}\")\n",
    "print(f\"Top 10% threshold for Die Linke support: {linke_threshold}\")\n",
    "\n",
    "# Define the function to classify respondents as extremists\n",
    "def classify_extremists(row):\n",
    "    if row['F6a_afdA1'] >= afd_threshold:\n",
    "        return 'far_right'\n",
    "    elif row['F6a_linkeA1'] >= linke_threshold:\n",
    "        return 'far_left'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the classification to the dataset\n",
    "data['extreme_political_attitudes'] = data.apply(classify_extremists, axis=1)\n",
    "\n",
    "# Filter dataset to include only individuals classified as extremists\n",
    "extremist_data = data[data['extreme_political_attitudes'].notnull()]\n",
    "\n",
    "# Select relevant columns for the new dataset\n",
    "output_columns = ['unique_id', 'date', 'extreme_political_attitudes', 'F6a_afdA1', 'F6a_linkeA1']\n",
    "filtered_dataset = extremist_data[output_columns]\n",
    "\n",
    "# Save the new dataset to a file\n",
    "output_file = 'extreme_political_attitudes_top10percent.xlsx'\n",
    "filtered_dataset.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Extreme political attitudes dataset saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset \n",
    "file_path = '/content/extreme_political_attitudes_top10percent.xlsx' \n",
    "extremist_data = pd.read_excel(file_path)\n",
    "extremist_data['date'] = pd.to_datetime(extremist_data['date'])\n",
    "extremist_data_clean = extremist_data.dropna(subset=['F6a_afdA1', 'F6a_linkeA1', 'extreme_political_attitudes'])\n",
    "\n",
    "# Function to perform Chow test\n",
    "def chow_test(y, X, split_index):\n",
    "    model_full = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    X1, y1 = X[:split_index], y[:split_index]\n",
    "    X2, y2 = X[split_index:], y[split_index:]\n",
    "    model_1 = sm.OLS(y1, sm.add_constant(X1)).fit()\n",
    "    model_2 = sm.OLS(y2, sm.add_constant(X2)).fit()\n",
    "\n",
    "    rss_full = model_full.ssr\n",
    "    rss_1, rss_2 = model_1.ssr, model_2.ssr\n",
    "    k = X.shape[1] + 1\n",
    "    n1, n2 = len(y1), len(y2)\n",
    "\n",
    "    chow_stat = ((rss_full - (rss_1 + rss_2)) / k) / ((rss_1 + rss_2) / (n1 + n2 - 2 * k))\n",
    "    p_value = 1 - f.cdf(chow_stat, k, n1 + n2 - 2 * k)\n",
    "\n",
    "    return chow_stat, p_value, model_1, model_2\n",
    "\n",
    "# Function to perform Chow tests for a specific response variable and event dates\n",
    "def perform_chow_tests(response_variable, event_dates, survey_data, group_name, hypothesis, event_type, window_size=60):\n",
    "    results = []\n",
    "    for event_date in event_dates:\n",
    "        start_date = event_date - timedelta(days=window_size)\n",
    "        end_date = event_date + timedelta(days=window_size)\n",
    "        window_data = survey_data[(survey_data['date'] >= start_date) & (survey_data['date'] <= end_date)]\n",
    "\n",
    "        y = window_data[response_variable]\n",
    "        X = window_data['date'].map(datetime.toordinal)  # Use time (date as ordinal) as predictor\n",
    "        X = X.values.reshape(-1, 1)  # Reshape to 2D array for regression\n",
    "        split_index = len(window_data[window_data['date'] < event_date])\n",
    "\n",
    "        if len(y) < 4 or split_index < 2 or split_index > (len(y) - 2):\n",
    "            chow_stat, p_value, change_magnitude, coverage = np.nan, np.nan, np.nan, np.nan\n",
    "        else:\n",
    "            chow_stat, p_value, model_1, model_2 = chow_test(y, X, split_index)\n",
    "            change_magnitude = abs(model_2.params.iloc[1] - model_1.params.iloc[1])  # Difference in slopes as a measure of change\n",
    "            coverage = linregress(X.flatten(), y).slope  # Local gradient over the full window\n",
    "\n",
    "        results.append({\n",
    "            'group': group_name,\n",
    "            'event_date': event_date.strftime('%Y-%m-%d'),\n",
    "            'hypothesis': hypothesis,\n",
    "            'Event': event_type,\n",
    "            'response_variable': response_variable,\n",
    "            'chow_stat': chow_stat,\n",
    "            'p_value': p_value,\n",
    "            'change_magnitude': change_magnitude,\n",
    "            'significant_change': p_value < 0.05,\n",
    "            'local_gradient': coverage\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define event dates grouped by event type for H8\n",
    "event_dates = {\n",
    "    \"energy crisis\": [datetime(2024, 1, 25), datetime(2023, 2, 7), datetime(2023, 10, 24), datetime(2023, 1, 12), datetime(2022, 12, 19)],\n",
    "    \"immigration crisis\": [datetime(2024, 6, 4), datetime(2023, 10, 5), datetime(2023, 5, 23)],\n",
    "    \"Ukraine war\": [datetime(2024, 7, 16), datetime(2024, 7, 17), datetime(2024, 7, 18)]\n",
    "}\n",
    "\n",
    "# Split the dataset based on extreme political attitudes (far-left and far-right)\n",
    "grouped_results = []\n",
    "\n",
    "for group in ['far_left', 'far_right']:\n",
    "    subset = extremist_data_clean[extremist_data_clean['extreme_political_attitudes'] == group]\n",
    "    if not subset.empty:\n",
    "        for event_type, dates in event_dates.items():\n",
    "            # Perform Chow tests for AfD and Die Linke support\n",
    "            afd_results = perform_chow_tests('F6a_afdA1', dates, subset, group, 'H8', event_type)\n",
    "            linke_results = perform_chow_tests('F6a_linkeA1', dates, subset, group, 'H8', event_type)\n",
    "            grouped_results.append(afd_results)\n",
    "            grouped_results.append(linke_results)\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "final_results = pd.concat(grouped_results, ignore_index=True)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "output_file = 'extreme_political_attitudes_chow_results_with_immigration_ukraine.xlsx'\n",
    "final_results.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Chow Test Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonstiges: Betrachtung des Dataset SOSECs\n",
    "Im Folgenden betrachten wir:\n",
    "- Wie viele unterschiedliche IDs es insgesamt gibt.\n",
    "- Wie viele unique_ids nur einmal, zweimal, dreimal oder Ã¶fter den Fragebogen beantwortet haben.\n",
    "- Wie viele unique_ids wir aus dem Datensatz entfernen mÃ¼ssen, weil ihre Antworten ungÃ¼ltig sind (zum Beispiel aufgrund von fehlenden oder falschen Werten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Lade das Datenset \n",
    "file_path = '/content/data_sample_700_SOSEC_dataset_germany_inTabellenform.csv'\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1', sep=';', on_bad_lines='skip')\n",
    "\n",
    "# 2. Definiere relevante Spalten, bei denen wir ungÃ¼ltige Antworten (99) und fehlende Werte unterscheiden wollen\n",
    "relevant_columns = ['F6mA1_1', 'F6a_linkeA1', 'F6a_afdA1', 'F6a_bswA1']\n",
    "\n",
    "# 3. Kopiere das Datenset, um es zu bereinigen\n",
    "data_cleaned = data.copy()\n",
    "\n",
    "# 4. ZÃ¤hle, wie oft 99 in den relevanten Spalten auftritt (als \"ungÃ¼ltige Antwort\")\n",
    "invalid_99_count = data_cleaned[relevant_columns].apply(lambda x: (x == 99).sum())\n",
    "\n",
    "# 5. Ersetze 99 mit NaN, um sie als fehlend zu behandeln\n",
    "data_cleaned[relevant_columns] = data_cleaned[relevant_columns].replace(99, pd.NA)\n",
    "\n",
    "# 6. ZÃ¤hle, wie oft tatsÃ¤chlich fehlende Werte (NaN) in den relevanten Spalten vorkommen\n",
    "missing_count = data_cleaned[relevant_columns].isna().sum()\n",
    "\n",
    "# 7. ZÃ¤hle die Anzahl der unique_idÂ´s, bei denen **alle** relevanten Spalten fehlen (also `NaN` sind)\n",
    "invalid_ids_all_missing = data_cleaned['unique_id'][data_cleaned[relevant_columns].isna().all(axis=1)].nunique()\n",
    "\n",
    "# 8. Analysiere, wie viele unique_idÂ´s einmal, zweimal, dreimal oder Ã¶fter den Fragebogen beantwortet haben\n",
    "response_counts = data_cleaned['unique_id'].value_counts()\n",
    "\n",
    "# Berechne, wie viele unique_ids den Fragebogen 1x, 2x, 3x oder Ã¶fter beantwortet haben\n",
    "once = (response_counts == 1).sum()\n",
    "twice = (response_counts == 2).sum()\n",
    "thrice = (response_counts == 3).sum()\n",
    "more_than_three = (response_counts > 3).sum()\n",
    "\n",
    "# 9. Gesamtzahl der unique_ids\n",
    "total_unique_ids = data_cleaned['unique_id'].nunique()\n",
    "\n",
    "# 10. Ergebnisse anzeigen\n",
    "print(f\"Gesamtzahl der unterschiedlichen unique_ids: {total_unique_ids}\")\n",
    "print(f\"Anzahl der unique_ids, die den Fragebogen nur einmal beantwortet haben: {once}\")\n",
    "print(f\"Anzahl der unique_ids, die den Fragebogen zweimal beantwortet haben: {twice}\")\n",
    "print(f\"Anzahl der unique_ids, die den Fragebogen dreimal beantwortet haben: {thrice}\")\n",
    "print(f\"Anzahl der unique_ids, die den Fragebogen mehr als dreimal beantwortet haben: {more_than_three}\")\n",
    "print(f\"Anzahl der unique_ids, die ausgeschlossen wurden, weil alle relevanten Fragen nicht beantwortet wurden: {invalid_ids_all_missing}\")\n",
    "\n",
    "# 11. HÃ¤ufigste GrÃ¼nde fÃ¼r den Ausschluss: Wie oft 99 oder fehlende Antworten vorkommen\n",
    "print(\"\\nHÃ¤ufigkeit von ungÃ¼ltigen Antworten (99) pro Frage:\")\n",
    "print(invalid_99_count)\n",
    "\n",
    "print(\"\\nHÃ¤ufigkeit von fehlenden Antworten (NaN) pro Frage:\")\n",
    "print(missing_count)\n",
    "\n",
    "# 12. Zusammenfassende Analyse (GrÃ¼nde fÃ¼r den Ausschluss)\n",
    "total_invalid = invalid_99_count + missing_count\n",
    "print(\"\\nZusammenfassende Analyse der AusschlussgrÃ¼nde:\")\n",
    "print(total_invalid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden betrachten wir, wie viele Menschen Ã¼ber den gesamten Zeitraum, die Fragen beantwortet haben zum Datenabgleich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Lade das Datenset (Pfad zum Datenset anpassen)\n",
    "file_path = '/content/data_sample_700_SOSEC_dataset_germany_inTabellenform.csv'\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1', sep=';', on_bad_lines='skip')\n",
    "\n",
    "# 2. Konvertiere die Spalte 'i_START' in ein Datumsformat\n",
    "data['i_START'] = pd.to_datetime(data['i_START'], errors='coerce')\n",
    "\n",
    "# 3. WÃ¤hle die relevanten Spalten (Fragen) aus, die analysiert werden sollen\n",
    "relevant_columns = ['F6mA1_1', 'F6a_linkeA1', 'F6a_afdA1', 'F6a_bswA1']\n",
    "\n",
    "# 4. Ersetze ungÃ¼ltige Antworten (z.B. 99) durch NaN, um diese nicht zu zÃ¤hlen\n",
    "for col in relevant_columns:\n",
    "    data[col] = data[col].replace(99, pd.NA)\n",
    "\n",
    "# 5. Gruppiere die Daten nach dem Zeitpunkt (i_START) und zÃ¤hle, wie viele Leute jede Frage beantwortet haben\n",
    "answers_over_time = data.groupby(data['i_START'].dt.to_period('M'))[relevant_columns].count()\n",
    "\n",
    "# 6. Plotten der Anzahl der Antworten Ã¼ber die Zeit\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for col in relevant_columns:\n",
    "    plt.plot(answers_over_time.index.to_timestamp(), answers_over_time[col], label=col)\n",
    "\n",
    "# 7. Plot-Details hinzufÃ¼gen\n",
    "plt.title('Anzahl der Antworten Ã¼ber die Zeit fÃ¼r jede Frage')\n",
    "plt.xlabel('Zeit')\n",
    "plt.ylabel('Anzahl der Antworten')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "# 8. Zeige den Plot\n",
    "plt.show()\n",
    "\n",
    "print(data['i_START'].head())\n",
    "print(data[relevant_columns].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonstiges: Clusteringversuch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def load_dataset(file_path: str, delimiter: str = ';', encoding: str = 'ISO-8859-1', low_memory: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Load the dataset and handle bad lines.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter, encoding=encoding, on_bad_lines='skip', low_memory=low_memory)\n",
    "        logging.info(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Strip leading/trailing whitespace from column names.\"\"\"\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "def convert_gender_to_numeric(df: pd.DataFrame, gender_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Convert the 'gender' column to numeric values.\"\"\"\n",
    "    gender_mapping = {1: 'mÃ¤nnlich', 2: 'weiblich', 3: 'anders', 4: 'mÃ¶chte ich nicht angeben'}\n",
    "    df[gender_col] = df[gender_col].map(gender_mapping).fillna('Unspecified')\n",
    "    df[gender_col] = df[gender_col].replace({\n",
    "        'mÃ¤nnlich': 1,\n",
    "        'weiblich': 2,\n",
    "        'anders': 3,\n",
    "        'mÃ¶chte ich nicht angeben': 4,\n",
    "        'Unspecified': -1  # or handle as NaN or remove rows if desired\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def reverse_scale(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    \"\"\"Reverse the scale of positively worded questions.\"\"\"\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "            df[column] = 8 - df[column]\n",
    "        else:\n",
    "            logging.warning(f\"Column {column} not found in dataset.\")\n",
    "    return df\n",
    "\n",
    "def calculate_weighted_mean(df: pd.DataFrame, positive_cols: list, negative_cols: list, min_answers_required_ratio: float = 0.7) -> pd.DataFrame:\n",
    "    \"\"\"Calculate the mean evaluation score only for answered questions.\"\"\"\n",
    "    relevant_cols = positive_cols + negative_cols\n",
    "    df[relevant_cols] = df[relevant_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    df['mean_evaluation_score'] = df[relevant_cols].mean(axis=1, skipna=True)\n",
    "    df['num_answers'] = df[relevant_cols].notna().sum(axis=1)\n",
    "    total_questions = len(relevant_cols)\n",
    "    threshold = total_questions * min_answers_required_ratio\n",
    "    df_clean = df[df['num_answers'] >= threshold].copy()\n",
    "    logging.info(f\"Filtered dataset to {df_clean.shape[0]} respondents.\")\n",
    "    return df_clean\n",
    "\n",
    "def categorize_responses(df: pd.DataFrame, negative_threshold: float = 4) -> pd.DataFrame:\n",
    "    \"\"\"Categorize respondents into 'Negative' and 'Others'.\"\"\"\n",
    "    df['evaluation_category'] = df['mean_evaluation_score'].apply(lambda x: 'Negative' if x < negative_threshold else 'Others')\n",
    "    return df\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, file_name: str):\n",
    "    \"\"\"Save the DataFrame to a CSV file.\"\"\"\n",
    "    try:\n",
    "        df.to_csv(file_name, index=False)\n",
    "        logging.info(f\"Data saved to {file_name}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving CSV: {e}\")\n",
    "\n",
    "def analyze_negative_respondent_characteristics(df, negative_ids, characteristics_cols):\n",
    "    \"\"\"\n",
    "    Analyze characteristics of respondents who gave negative evaluations.\n",
    "    Cluster respondents based on these characteristics.\n",
    "    \"\"\"\n",
    "    negative_df = df[df['unique_id'].isin(negative_ids)].copy()\n",
    "\n",
    "    if negative_df.empty:\n",
    "        logging.warning(\"No respondents found with negative evaluations.\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Number of negative evaluations found: {len(negative_df)}\")\n",
    "    characteristics_df = negative_df[characteristics_cols].dropna()\n",
    "\n",
    "    if characteristics_df.empty:\n",
    "        logging.warning(\"No data available after selecting characteristics and dropping missing values.\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Rows after dropping NaNs: {len(characteristics_df)}\")\n",
    "\n",
    "    # Standardize the features for better clustering performance\n",
    "    scaler = StandardScaler()\n",
    "    characteristics_scaled = scaler.fit_transform(characteristics_df)\n",
    "\n",
    "    # Use K-Means clustering to group respondents based on their characteristics\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    kmeans.fit(characteristics_scaled)\n",
    "\n",
    "    # Assign cluster labels back to the original DataFrame\n",
    "    negative_df.loc[characteristics_df.index, 'cluster'] = kmeans.labels_\n",
    "\n",
    "    logging.info(f\"Cluster labels assigned. Number of respondents in each cluster:\\n{negative_df['cluster'].value_counts(dropna=False)}\")\n",
    "\n",
    "    return negative_df\n",
    "\n",
    "def visualize_pca_clusters(df, cluster_column, characteristics_scaled):\n",
    "    \"\"\"\n",
    "    Visualizes clusters using PCA to reduce dimensionality to 2 components.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=2)\n",
    "    components = pca.fit_transform(characteristics_scaled)\n",
    "    pca_df = pd.DataFrame(data=components, columns=['PC1', 'PC2'])\n",
    "    pca_df[cluster_column] = df.loc[df.index, cluster_column]\n",
    "    pca_df[cluster_column] = pca_df[cluster_column].astype('category')\n",
    "\n",
    "    logging.info(f\"Unique clusters in data: {pca_df[cluster_column].unique()}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='PC1', y='PC2', hue=cluster_column, data=pca_df, palette='viridis')\n",
    "    plt.title('PCA Plot of Clusters')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.show()\n",
    "\n",
    "def print_pca_components(pca, features):\n",
    "    \"\"\"\n",
    "    Prints the PCA components with the weights of each feature.\n",
    "\n",
    "    Parameters:\n",
    "    - pca: fitted PCA object\n",
    "    - features: list of feature names (original variables in the dataset)\n",
    "    \"\"\"\n",
    "    # Get the components (loadings) and create a DataFrame\n",
    "    pca_components = pd.DataFrame(pca.components_, columns=features)\n",
    "\n",
    "    # Add a description of how much variance each component explains\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    print(\"\\nPrincipal Component Analysis (PCA) Loadings:\")\n",
    "    for i, (comp, var) in enumerate(zip(pca_components.iterrows(), explained_variance), 1):\n",
    "        print(f\"\\nPrincipal Component {i} explains {var:.2%} of the variance:\")\n",
    "        print(comp[1])\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    file_path = '/content/data_sample_700_SOSEC_dataset_germany_inTabellenform.csv'\n",
    "    output_file_cleaned = 'cleaned_responses_germany.csv'\n",
    "    output_file_clustered = 'clustered_negative_evaluation_characteristics.csv'\n",
    "    exclude_cols = ['i_NUMBER', 'i_START', 'i_END', 'i_TIME', 'wave_first']\n",
    "\n",
    "    # Define columns to reverse scale\n",
    "    positive_cols = ['F1A1_1', 'F1A5_1', 'F1A5_2', 'F1A10_1', 'F1A14_1']\n",
    "    negative_cols = ['F1A2_1', 'F1A3_1', 'F1A4_1']\n",
    "\n",
    "    df = load_dataset(file_path)\n",
    "\n",
    "    if df is not None:\n",
    "        df = clean_column_names(df)\n",
    "        df = df.drop(columns=exclude_cols, errors='ignore')\n",
    "\n",
    "        df = reverse_scale(df, positive_cols)\n",
    "        df_clean = calculate_weighted_mean(df, positive_cols, negative_cols, min_answers_required_ratio=0.7)\n",
    "        df_clean = categorize_responses(df_clean, negative_threshold=4)\n",
    "        save_to_csv(df_clean, output_file_cleaned)\n",
    "\n",
    "        negative_ids = df_clean[df_clean['evaluation_category'] == 'Negative']['unique_id']\n",
    "\n",
    "        if negative_ids.empty:\n",
    "            logging.warning(\"No respondents with negative evaluations found.\")\n",
    "            return\n",
    "\n",
    "        # Convert gender column 'F7a' to numeric before clustering\n",
    "        df_clean = convert_gender_to_numeric(df_clean, 'F7a')\n",
    "\n",
    "        characteristics_cols = ['F7a', 'F7bA1', 'F7cA1', 'F7d', 'einkommen', 'F7g', 'F7h', 'F7i', 'F7j']\n",
    "        negative_characteristics_df = analyze_negative_respondent_characteristics(df_clean, negative_ids, characteristics_cols)\n",
    "\n",
    "        if negative_characteristics_df is not None:\n",
    "            save_to_csv(negative_characteristics_df, output_file_clustered)\n",
    "            print(df[characteristics_cols].describe())\n",
    "\n",
    "            # Visualize the clusters using PCA\n",
    "            characteristics_scaled = StandardScaler().fit_transform(negative_characteristics_df[characteristics_cols])\n",
    "            visualize_pca_clusters(negative_characteristics_df, 'cluster', characteristics_scaled)\n",
    "\n",
    "            # Print PCA components\n",
    "            pca = PCA(n_components=2)\n",
    "            pca.fit(characteristics_scaled)\n",
    "            print_pca_components(pca, characteristics_cols)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonstiges: Grafiken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_political_orientation_change(df, event_dates, question_col='F6mA1_1'):\n",
    "    \"\"\"\n",
    "    Analyze changes in political orientation around specific event dates.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing participant responses over time\n",
    "    - event_dates: List of relevant event dates\n",
    "    - question_col: Column name for political orientation responses\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame showing change in political orientation around event dates\n",
    "    \"\"\"\n",
    "    # Convert event_dates to datetime (only dates, no time)\n",
    "    event_dates = pd.to_datetime(event_dates, format='%Y-%m-%d')\n",
    "\n",
    "    # Convert i_START to datetime using the correct format (keep full Timestamp)\n",
    "    df['i_START'] = pd.to_datetime(df['i_START'], format='%d.%m.%y %H:%M', errors='coerce')\n",
    "\n",
    "    # Ensure 'unique_id' is treated as a string for easier manipulation\n",
    "    df['unique_id'] = df['unique_id'].astype(str)\n",
    "\n",
    "    # Initialize a dictionary to store changes in political orientation\n",
    "    changes = {'unique_id': [], 'event_date': [], 'before_orientation': [], 'after_orientation': [], 'change': []}\n",
    "\n",
    "    # Loop over each event date\n",
    "    for event in event_dates:\n",
    "        # Filter responses before and after the event\n",
    "        before_event = df[df['i_START'] < event]\n",
    "        after_event = df[df['i_START'] >= event]\n",
    "\n",
    "        # Loop over each participant\n",
    "        for uid in df['unique_id'].unique():\n",
    "            # Get the latest response before the event and the earliest after the event\n",
    "            before_responses = before_event[before_event['unique_id'] == uid].sort_values(by='i_START')\n",
    "            after_responses = after_event[after_event['unique_id'] == uid].sort_values(by='i_START')\n",
    "\n",
    "            # Only proceed if we have both before and after responses\n",
    "            if not before_responses.empty and not after_responses.empty:\n",
    "                before_response = before_responses[question_col].iloc[-1]  # Last before event\n",
    "                after_response = after_responses[question_col].iloc[0]    # First after event\n",
    "\n",
    "                # Calculate change and append to the results dictionary\n",
    "                change = after_response - before_response\n",
    "                changes['unique_id'].append(uid)\n",
    "                changes['event_date'].append(event)\n",
    "                changes['before_orientation'].append(before_response)\n",
    "                changes['after_orientation'].append(after_response)\n",
    "                changes['change'].append(change)\n",
    "\n",
    "    # Convert the changes dictionary to a DataFrame\n",
    "    changes_df = pd.DataFrame(changes)\n",
    "\n",
    "    return changes_df\n",
    "\n",
    "# Example Usage\n",
    "event_dates = ['2024-01-15']  # Example event dates\n",
    "\n",
    "\n",
    "\n",
    "file_path = '/content/data_sample_700_SOSEC_dataset_germany_inTabellenform.csv'  # file path\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path, delimiter=';', encoding='ISO-8859-1', on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "# Analyze political orientation changes around events\n",
    "orientation_changes = analyze_political_orientation_change(df, event_dates, question_col='F6mA1_1')\n",
    "\n",
    "print(orientation_changes.head())  # Check the first few rows\n",
    "print(orientation_changes['change'].describe())  # Summary of the change column\n",
    "print(df['i_START'].head())  # Check if dates are properly parsed\n",
    "print(df['i_START'].describe())  # Summary of date ranges\n",
    "for event in event_dates:\n",
    "    before_event = df[df['i_START'] < event]\n",
    "    after_event = df[df['i_START'] >= event]\n",
    "\n",
    "    print(f\"Event: {event}\")\n",
    "    print(f\"Before event: {len(before_event)} responses\")\n",
    "    print(f\"After event: {len(after_event)} responses\")\n",
    "print(df['F6mA1_1'].isnull().sum())  # Count null values\n",
    "print(df['F6mA1_1'].dropna().describe())  # Summary of valid responses\n",
    "\n",
    "\n",
    "# Visualize the changes\n",
    "plt.hist(orientation_changes['change'], bins=10)\n",
    "plt.title('Distribution of Political Orientation Changes Around Events')\n",
    "plt.xlabel('Change in Political Orientation (Scale 0-10)')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_party_approval_change(df, event_dates, question_col):\n",
    "    \"\"\"\n",
    "    Analyze changes in party approval around specific event dates.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing participant responses over time\n",
    "    - event_dates: List of relevant event dates\n",
    "    - question_col: Column name for party approval responses\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame showing change in party approval around event dates\n",
    "    \"\"\"\n",
    "    event_dates = pd.to_datetime(event_dates, format='%Y-%m-%d')\n",
    "    df['i_START'] = pd.to_datetime(df['i_START'], format='%d.%m.%y %H:%M', errors='coerce')\n",
    "    df['unique_id'] = df['unique_id'].astype(str)\n",
    "\n",
    "    changes = {'unique_id': [], 'event_date': [], 'before_approval': [], 'after_approval': [], 'change': []}\n",
    "\n",
    "    for event in event_dates:\n",
    "        before_event = df[df['i_START'] < event]\n",
    "        after_event = df[df['i_START'] >= event]\n",
    "\n",
    "        for uid in df['unique_id'].unique():\n",
    "            before_responses = before_event[before_event['unique_id'] == uid].sort_values(by='i_START')\n",
    "            after_responses = after_event[after_event['unique_id'] == uid].sort_values(by='i_START')\n",
    "\n",
    "            if not before_responses.empty and not after_responses.empty:\n",
    "                before_response = before_responses[question_col].iloc[-1]\n",
    "                after_response = after_responses[question_col].iloc[0]\n",
    "\n",
    "                change = after_response - before_response\n",
    "                if abs(change) >= 2:  # Only consider changes of 2 points or more\n",
    "                    changes['unique_id'].append(uid)\n",
    "                    changes['event_date'].append(event)\n",
    "                    changes['before_approval'].append(before_response)\n",
    "                    changes['after_approval'].append(after_response)\n",
    "                    changes['change'].append(change)\n",
    "\n",
    "    changes_df = pd.DataFrame(changes)\n",
    "    return changes_df\n",
    "\n",
    "def plot_changes(changes_df, party_name):\n",
    "    \"\"\"\n",
    "    Plot the changes in party approval.\n",
    "\n",
    "    Parameters:\n",
    "    - changes_df: DataFrame containing changes in approval\n",
    "    - party_name: Name of the political party\n",
    "    \"\"\"\n",
    "    if not changes_df.empty:\n",
    "        plt.hist(changes_df['change'], bins=10)\n",
    "        plt.title(f'Change in Approval for {party_name} Around Events')\n",
    "        plt.xlabel('Change in Approval (Scale 0-10)')\n",
    "        plt.ylabel('Number of Participants')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No significant changes (>= 2 points) for {party_name} to plot.\")\n",
    "\n",
    "# Example event dates\n",
    "event_dates = ['2024-01-25', '2023-02-07', '2023-10-24', '2023-01-12','2022-12-19', '2024-06-04', '2023-10-05', '2023-05-23', '2024-07-16', '2024-07-17','2024-07-18'] # Daten zur energy crisis\n",
    "\n",
    "# Load dataset\n",
    "file_path = '/content/data_sample_700_SOSEC_dataset_germany_inTabellenform.csv'\n",
    "df = pd.read_csv(file_path, delimiter=';', encoding='ISO-8859-1', on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "# Analyze and plot changes for \"Die Linke\"\n",
    "linke_changes = analyze_party_approval_change(df, event_dates, question_col='F6a_linkeA1')\n",
    "plot_changes(linke_changes, \"Die Linke\")\n",
    "\n",
    "# Analyze and plot changes for \"AFD\"\n",
    "afd_changes = analyze_party_approval_change(df, event_dates, question_col='F6a_afdA1')\n",
    "plot_changes(afd_changes, \"AFD\")\n",
    "\n",
    "# Analyze and plot changes for \"BSW\"\n",
    "bsw_changes = analyze_party_approval_change(df, event_dates, question_col='F6a_bswA1')\n",
    "plot_changes(bsw_changes, \"BSW\")\n",
    "\n",
    "# Check if there is valid data to plot\n",
    "if not bsw_changes['change'].isnull().all():\n",
    "    plot_changes(bsw_changes, \"BSW\")\n",
    "else:\n",
    "    print(\"No valid data for plotting the 'BSW' changes.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your dataset\n",
    "file_path = '/content/data_sample_700_SOSEC_dataset_germany_inTabellenform.xlsm'  # Replace with your local file path\n",
    "data_raw = pd.read_excel(file_path, engine='openpyxl', header=None)\n",
    "\n",
    "# Extract headers and reassign to data\n",
    "headers = data_raw.iloc[0, 0].split(\";\")\n",
    "data = data_raw.iloc[1:, 0].str.split(\";\", expand=True)\n",
    "data.columns = headers\n",
    "\n",
    "# Extract relevant columns for AfD\n",
    "relevant_data_afd = data[['i_START', 'F6a_afdA1']].copy()\n",
    "relevant_data_afd['i_START'] = pd.to_datetime(relevant_data_afd['i_START'], errors='coerce')\n",
    "relevant_data_afd['F6a_afdA1'] = pd.to_numeric(relevant_data_afd['F6a_afdA1'], errors='coerce')\n",
    "\n",
    "# Extract relevant columns for Die Linke\n",
    "relevant_data_linke = data[['i_START', 'F6a_linkeA1']].copy()\n",
    "relevant_data_linke['i_START'] = pd.to_datetime(relevant_data_linke['i_START'], errors='coerce')\n",
    "relevant_data_linke['F6a_linkeA1'] = pd.to_numeric(relevant_data_linke['F6a_linkeA1'], errors='coerce')\n",
    "\n",
    "# Filter out missing and invalid values\n",
    "filtered_data_afd = relevant_data_afd[\n",
    "    (relevant_data_afd['F6a_afdA1'] >= 0) & (relevant_data_afd['F6a_afdA1'] <= 100)\n",
    "]\n",
    "filtered_data_linke = relevant_data_linke[\n",
    "    (relevant_data_linke['F6a_linkeA1'] >= 0) & (relevant_data_linke['F6a_linkeA1'] <= 100)\n",
    "]\n",
    "\n",
    "# Group data weekly for AfD\n",
    "filtered_data_afd['Weekly'] = filtered_data_afd['i_START'].dt.to_period('W')\n",
    "weekly_avg_afd = filtered_data_afd.groupby('Weekly')['F6a_afdA1'].mean().reset_index()\n",
    "weekly_avg_afd['Weekly'] = weekly_avg_afd['Weekly'].apply(lambda x: x.start_time)\n",
    "\n",
    "# Group data weekly for Die Linke\n",
    "filtered_data_linke['Weekly'] = filtered_data_linke['i_START'].dt.to_period('W')\n",
    "weekly_avg_linke = filtered_data_linke.groupby('Weekly')['F6a_linkeA1'].mean().reset_index()\n",
    "weekly_avg_linke['Weekly'] = weekly_avg_linke['Weekly'].apply(lambda x: x.start_time)\n",
    "\n",
    "# Apply smoothing\n",
    "weekly_avg_afd['Smoothed Rolling Average'] = weekly_avg_afd['F6a_afdA1'].rolling(window=8).mean()\n",
    "weekly_avg_linke['Smoothed Rolling Average'] = weekly_avg_linke['F6a_linkeA1'].rolling(window=8).mean()\n",
    "\n",
    "# Define event dates\n",
    "event_dates = [\n",
    "    pd.Timestamp('2024-01-25'),\n",
    "    pd.Timestamp('2023-02-07'),\n",
    "    pd.Timestamp('2023-10-24'),\n",
    "    pd.Timestamp('2023-01-12'),\n",
    "    pd.Timestamp('2022-12-19')\n",
    "]\n",
    "\n",
    "# Plot the AfD and Die Linke approval graphs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(weekly_avg_afd['Weekly'], weekly_avg_afd['Smoothed Rolling Average'], color='blue', label='AfD Smoothed Approval')\n",
    "plt.plot(weekly_avg_linke['Weekly'], weekly_avg_linke['Smoothed Rolling Average'], color='green', label='Die Linke Smoothed Approval')\n",
    "\n",
    "\n",
    "# Set y-axis scale to 0-11\n",
    "plt.ylim(0, 11)\n",
    "\n",
    "# Labeling and title\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Approval (%)', fontsize=12)\n",
    "plt.title('Smoothed AfD and Die Linke Approval Over Time with Full Scale (0-11)', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot data with Chow test regressions and visually display local gradients at event dates\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Scatter plot all data points for context\n",
    "plt.scatter(weekly_filtered_afd_approval['Weekly'], weekly_filtered_afd_approval['F6a_afdA1'],\n",
    "            color='lightgray', s=10, label='All Data Points')\n",
    "\n",
    "# Loop through each Chow test event\n",
    "for event_date in event_dates:\n",
    "    window_start = event_date - pd.Timedelta(days=60)\n",
    "    window_end = event_date + pd.Timedelta(days=60)\n",
    "\n",
    "    # Filter data for the 60-day window\n",
    "    window_data = weekly_filtered_afd_approval[\n",
    "        (weekly_filtered_afd_approval['Weekly'] >= window_start) &\n",
    "        (weekly_filtered_afd_approval['Weekly'] <= window_end)\n",
    "    ]\n",
    "\n",
    "    # Filter data before and after the event date\n",
    "    before_event = window_data[window_data['Weekly'] < event_date]\n",
    "    after_event = window_data[window_data['Weekly'] >= event_date]\n",
    "\n",
    "    # Regression line before the event\n",
    "    if not before_event.empty:\n",
    "        x_before = np.array((before_event['Weekly'] - before_event['Weekly'].min()).dt.days).reshape(-1, 1)\n",
    "        y_before = before_event['F6a_afdA1']\n",
    "        model_before = LinearRegression().fit(x_before, y_before)\n",
    "        y_pred_before = model_before.predict(x_before)\n",
    "        plt.plot(before_event['Weekly'], y_pred_before, linestyle='--', color='green', linewidth=2, label=f'Regression Before {event_date.date()}')\n",
    "\n",
    "    # Regression line after the event\n",
    "    if not after_event.empty:\n",
    "        x_after = np.array((after_event['Weekly'] - after_event['Weekly'].min()).dt.days).reshape(-1, 1)\n",
    "        y_after = after_event['F6a_afdA1']\n",
    "        model_after = LinearRegression().fit(x_after, y_after)\n",
    "        y_pred_after = model_after.predict(x_after)\n",
    "        plt.plot(after_event['Weekly'], y_pred_after, linestyle='--', color='blue', linewidth=2, label=f'Regression After {event_date.date()}')\n",
    "\n",
    "    # Add a vertical line for the event date\n",
    "    plt.axvline(event_date, color='red', linestyle='--', alpha=0.8, label=f'Chow Test: {event_date.date()}')\n",
    "\n",
    "    # Calculate and annotate the local gradient at the event date\n",
    "    if not before_event.empty and not after_event.empty:\n",
    "        # Local gradient as the slope of the difference between the first point after and the last point before\n",
    "        y_diff = y_after.iloc[0] - y_before.iloc[-1]\n",
    "        x_diff = (after_event['Weekly'].iloc[0] - before_event['Weekly'].iloc[-1]).days\n",
    "        local_gradient = y_diff / x_diff if x_diff != 0 else 0\n",
    "\n",
    "        # Annotate the gradient\n",
    "        plt.text(event_date, window_data['F6a_afdA1'].max(),\n",
    "                 f\"Gradient: {local_gradient:.3f}\", fontsize=10, color='purple', rotation=90, verticalalignment='bottom')\n",
    "\n",
    "# Styling\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Average AfD Approval', fontsize=12)\n",
    "plt.title('Linear Regression with Chow Tests and Local Gradients at Event Dates', fontsize=14)\n",
    "plt.legend(fontsize=10, loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
